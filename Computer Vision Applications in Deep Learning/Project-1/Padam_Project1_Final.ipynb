{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://commondatastorage.googleapis.com/books1000/notMNIST_small.tar.gz\n",
    "\n",
    "# import tarfile\n",
    "# with tarfile.open('notMNIST_small.tar.gz', 'r:gz') as tar:\n",
    "#     tar.extractall()\n",
    "\n",
    "# !ls notMNIST_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.image as mp\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct  8 23:49:52 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:08:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             54W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off |   00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             54W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off |   00000000:88:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             54W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off |   00000000:8C:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             54W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "# GPU device\n",
    "gpu_id = 3\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notMNIST_read(number_of_files, file):\n",
    "    c = 10  # classes A-J (10)\n",
    "    m = number_of_files\n",
    "\n",
    "    trainX = np.zeros((m, 28 * 28), dtype=np.uint8)  # images are 28*28\n",
    "    trainY = np.zeros((m, c), dtype=np.uint8)\n",
    "\n",
    "    ix = 0\n",
    "    for root, dirs, files in os.walk(file):\n",
    "        for f in files:\n",
    "            if f.endswith('.png'):\n",
    "                try:\n",
    "                    img = mp.imread(os.path.join(root, f))\n",
    "                    if img.ndim == 3 and img.shape[2] == 4:\n",
    "                        img = img[:, :, :3]  # Convert RGBA to RGB\n",
    "                    trainX[ix, :] = img.reshape(-1)\n",
    "                    folder = os.path.basename(root)  #os.path.split(root)[-1]\n",
    "                    letter = ord(folder) - ord('A')  # A=0, B=1,..., J=9\n",
    "                    trainY[ix, letter] = 1\n",
    "                    print(f\"Processing letter {folder}, file number {ix}\\r\", end='', flush=True)\n",
    "                    ix += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {f}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    save_name = 'notMNIST_' + ('Large' if m == 529114 else 'small') + '_train'\n",
    "    np.save(f'{save_name}X.npy', trainX[:ix])\n",
    "    np.save(f'{save_name}Y.npy', trainY[:ix])\n",
    "\n",
    "    return trainX, trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png: not a PNG file\n",
      "Error processing file RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png: not a PNG file\n",
      "Processing letter B, file number 18723\r"
     ]
    }
   ],
   "source": [
    "trainX,trainY = notMNIST_read(18724,r\"/home/pthapa2/snap/padam/Project1/notMNIST_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the notMNIST dataset\n",
    "trainX = np.load('notMNIST_small_trainX.npy')\n",
    "trainY = np.load('notMNIST_small_trainY.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainX/255\n",
    "Y = trainY/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18724, 784), 0.0, 0.00392156862745098, (18724, 10), 0.0, 1.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.min(), X.max(), Y.shape, Y.min(), Y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1872., 1873., 1873., 1873., 1873., 1872., 1872., 1872., 1872.,\n",
       "       1872.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the input data\n",
    "def standardize(X):\n",
    "    X = X.astype(np.float64)\n",
    "    Xm = np.mean(X, axis=1, keepdims=True)\n",
    "    Xstd = np.std(X, axis=1, keepdims=True)\n",
    "    Xstd[Xstd == 0] = 1  # Prevent division by zero\n",
    "    X -= Xm\n",
    "    X /= Xstd\n",
    "    return X\n",
    "\n",
    "trainX = standardize(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom train-validation-test split function\n",
    "def trainsplit(X, Y, percentV=0.2, percentT=0.2): #Try 0.15\n",
    "    m = X.shape[0]\n",
    "    ix = list(range(m))\n",
    "    np.random.shuffle(ix)  # Randomize the DB index\n",
    "    X = X[ix, :]\n",
    "    Y = Y[ix, :]\n",
    "    \n",
    "    m_train = int(np.floor(m * (1 - percentV - percentT)))  # Cut off for training\n",
    "    m_val = m_train + int(np.floor(m * percentV))            # Cut off for validation data\n",
    "    \n",
    "    X_train = X[:m_train, :]\n",
    "    X_val = X[m_train:m_val, :]\n",
    "    X_test = X[m_val:, :]\n",
    "    \n",
    "    Y_train = Y[:m_train]\n",
    "    Y_val = Y[m_train:m_val]\n",
    "    Y_test = Y[m_val:]\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = trainsplit(trainX, trainY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "Larger batch sizes require more memory and may cause out-of-memory errors.\n",
    "\n",
    "Larger batch sizes can be computationally efficient but may slow down beyond a point.\n",
    "\n",
    "Smaller batch sizes: more noise, escape local minima, better generalization.\n",
    "\n",
    "Larger batch sizes: stable gradients, sharper minima, may affect generalization.\n",
    "\n",
    "Increase batch size ⇒ adjust learning rate accordingly.\n",
    "\n",
    "Larger batch sizes often require higher learning rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert standardized data to PyTorch tensors and move to device\n",
    "trainX_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "trainY_tensor = torch.tensor(Y_train.argmax(axis=1), dtype=torch.long).to(device)\n",
    "validX_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "validY_tensor = torch.tensor(Y_val.argmax(axis=1), dtype=torch.long).to(device)\n",
    "testX_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "testY_tensor = torch.tensor(Y_test.argmax(axis=1), dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for each dataset split\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(TensorDataset(trainX_tensor, trainY_tensor), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(validX_tensor, validY_tensor), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(testX_tensor, testY_tensor), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 11234\n",
      "Validation data length: 3744\n",
      "Test data length: 3746\n"
     ]
    }
   ],
   "source": [
    "# Print lengths of datasets\n",
    "print(f\"Training data length: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation data length: {len(valid_loader.dataset)}\")\n",
    "print(f\"Test data length: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(12, 12))\n",
    "fig.suptitle(\"Random Images from notMNIST Dataset\", fontsize=16)\n",
    "\n",
    "# Iterate over axes and display random images\n",
    "for ax in axes.flat:\n",
    "    img, label = next(iter(train_loader))  # Get a random batch\n",
    "    idx = np.random.randint(len(img))      # Random index within the batch\n",
    "    ax.imshow(img[idx].cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f\"Label: {chr(65 + label[idx].item())}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report\n",
    "### 1. Database: The database consists notMNIST image files, similar to MNIST but with letters A-J instead of digits.\n",
    "### 2. Input Data: The input data consists of 28x28 grayscale images in PNG format.\n",
    "### 3. Input Type: The input is a float64 data type. (dtype('float64'))\n",
    "### 4. Input Dimensions: Images resized to (28, 28) pixels, flattened to 784-dimensional vectors.\n",
    "### 5. Training Set: notMNIST_small, 11234 images in training set.\n",
    "### 6. Test Set: 3744 for the validation set, 3746 for the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: 4 Layered Network with 1 Input, 2 hidden layers, and 1 output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class Network121(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(Network121, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network121(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an instance of the model\n",
    "input_size = 28 * 28\n",
    "hidden_size1 = 512\n",
    "hidden_size2 = 128\n",
    "output_size = 10\n",
    "\n",
    "model1 = Network121(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:3\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 512]         401,920\n",
      "            Linear-2               [-1, 1, 128]          65,664\n",
      "            Linear-3                [-1, 1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 468,874\n",
      "Trainable params: 468,874\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 1.79\n",
      "Estimated Total Size (MB): 1.80\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model is on device: {next(model1.parameters()).device}\")\n",
    "dummy_input = torch.randn(1, input_size).to(device)\n",
    "summary(model1, input_size=(1, input_size), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, label):\n",
    "    pred_classes = pred.argmax(dim=1)\n",
    "    # Comparing with true class indices directly\n",
    "    return (pred_classes == label).type(torch.float).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, metric, metric_name='batch accuracy'):\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, Y) in enumerate(dataloader, 1):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        Yhat = model(X)  # Forward pass through the model\n",
    "        loss = loss_fn(Yhat, Y)\n",
    "        loss.backward()  # Backpropagation - computing gradients\n",
    "        optimizer.step()  # Updating model weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calc. and printing metrics after every batch\n",
    "        avg_loss = total_loss / batch\n",
    "        metric_value = metric(Yhat, Y)\n",
    "        print(f\"Batch: {batch:>5d}/{num_batches}, Avg_loss: {avg_loss:>7f}, {metric_name}: {metric_value:5f}\", end='\\r')\n",
    "    \n",
    "    print()  \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, metric, metric_name = 'accuracy'):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, metric_T = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            Yhat = model(X)\n",
    "            test_loss += loss_fn(Yhat, Y).item()\n",
    "            metric_T += metric(Yhat, Y)* len(Y)\n",
    "    test_loss /= num_batches\n",
    "    metric_T /= size\n",
    "    print(f\"loss: {test_loss:>7f}, metric: { metric_T:>5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 2.301670, batch accuracy: 0.132653\n",
      "Validation:\n",
      "loss: 2.300538, metric: 0.091880\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 2.297984, batch accuracy: 0.316327\n",
      "Validation:\n",
      "loss: 2.295408, metric: 0.131677\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 2.286581, batch accuracy: 0.163265\n",
      "Validation:\n",
      "loss: 2.271729, metric: 0.179220\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 2.226163, batch accuracy: 0.265306\n",
      "Validation:\n",
      "loss: 2.183465, metric: 0.311432\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 2.117010, batch accuracy: 0.418367\n",
      "Validation:\n",
      "loss: 2.066136, metric: 0.416667\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 2.003533, batch accuracy: 0.438776\n",
      "Validation:\n",
      "loss: 1.971583, metric: 0.555288\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.926767, batch accuracy: 0.591837\n",
      "Validation:\n",
      "loss: 1.908268, metric: 0.580395\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.877638, batch accuracy: 0.622449\n",
      "Validation:\n",
      "loss: 1.871228, metric: 0.638889\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.844344, batch accuracy: 0.520408\n",
      "Validation:\n",
      "loss: 1.842884, metric: 0.658120\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.817875, batch accuracy: 0.673469\n",
      "Validation:\n",
      "loss: 1.823825, metric: 0.667468\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.801098, batch accuracy: 0.663265\n",
      "Validation:\n",
      "loss: 1.811745, metric: 0.666400\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.789898, batch accuracy: 0.663265\n",
      "Validation:\n",
      "loss: 1.803660, metric: 0.669605\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.755748, batch accuracy: 0.734694\n",
      "Validation:\n",
      "loss: 1.742610, metric: 0.764690\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.699112, batch accuracy: 0.775510\n",
      "Validation:\n",
      "loss: 1.702056, metric: 0.789530\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.670258, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.681689, metric: 0.814904\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.652665, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.670418, metric: 0.820246\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.640748, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.663027, metric: 0.822917\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.631722, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.657807, metric: 0.825053\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.624630, batch accuracy: 0.806122\n",
      "Validation:\n",
      "loss: 1.652375, metric: 0.824519\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.618532, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.651366, metric: 0.822115\n",
      "\n",
      "Testing:\n",
      "loss: 1.634284, metric: 0.842232\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "optimizer = optim.SGD(model1.parameters(), lr = 0.5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model1, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model1, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model1, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: 6 Layered Network with 1 Input, 4 hidden layers, and 1 output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Task 2 in a modular way as of Task 1\n",
    "class Network141(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(Network141, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features=layer_sizes[0], out_features=layer_sizes[1]) # Input Layer to First Hidden Layer\n",
    "        self.L2 = nn.Linear(in_features=layer_sizes[1], out_features=layer_sizes[2])\n",
    "        self.L3 = nn.Linear(in_features=layer_sizes[2], out_features=layer_sizes[3])\n",
    "        self.L4 = nn.Linear(in_features=layer_sizes[3], out_features=layer_sizes[4])\n",
    "        self.out = nn.Linear(in_features=layer_sizes[4], out_features=layer_sizes[5]) # Fourth Hidden Layer to Output Layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.L1.weight.dtype)       # Ensuring input tensor is of the same data type as model weights\n",
    "        L1 = torch.sigmoid(self.L1(x))       # Pass through First Hidden Layer with Sigmoid Activation\n",
    "        L2 = torch.sigmoid(self.L2(L1)) \n",
    "        L3 = torch.sigmoid(self.L3(L2))\n",
    "        L4 = torch.sigmoid(self.L4(L3))\n",
    "        out = F.softmax(self.out(L4), dim=1) # Pass through Output Layer with Softmax Activation for Probabilities\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 2.303007, batch accuracy: 0.040816\n",
      "Validation:\n",
      "loss: 2.303067, metric: 0.097222\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 2.302749, batch accuracy: 0.142857\n",
      "Validation:\n",
      "loss: 2.303158, metric: 0.096154\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 2.302742, batch accuracy: 0.061224\n",
      "Validation:\n",
      "loss: 2.303150, metric: 0.091880\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 2.302772, batch accuracy: 0.071429\n",
      "Validation:\n",
      "loss: 2.303457, metric: 0.097222\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 2.302784, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.303361, metric: 0.097222\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 2.302688, batch accuracy: 0.102041\n",
      "Validation:\n",
      "loss: 2.303166, metric: 0.097222\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 2.302669, batch accuracy: 0.102041\n",
      "Validation:\n",
      "loss: 2.303150, metric: 0.091880\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 2.302701, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.303258, metric: 0.097222\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 2.302762, batch accuracy: 0.081633\n",
      "Validation:\n",
      "loss: 2.303079, metric: 0.091880\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 2.302660, batch accuracy: 0.102041\n",
      "Validation:\n",
      "loss: 2.303505, metric: 0.091880\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 2.302708, batch accuracy: 0.122449\n",
      "Validation:\n",
      "loss: 2.303237, metric: 0.097222\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 2.302708, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.303297, metric: 0.091880\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 2.302688, batch accuracy: 0.153061\n",
      "Validation:\n",
      "loss: 2.303264, metric: 0.097222\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 2.302707, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.303400, metric: 0.091880\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 2.302723, batch accuracy: 0.142857\n",
      "Validation:\n",
      "loss: 2.303460, metric: 0.091880\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 2.302672, batch accuracy: 0.071429\n",
      "Validation:\n",
      "loss: 2.303291, metric: 0.096154\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 2.302641, batch accuracy: 0.061224\n",
      "Validation:\n",
      "loss: 2.303282, metric: 0.096154\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 2.302683, batch accuracy: 0.071429\n",
      "Validation:\n",
      "loss: 2.303544, metric: 0.096154\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 2.302717, batch accuracy: 0.173469\n",
      "Validation:\n",
      "loss: 2.303402, metric: 0.096154\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 2.302689, batch accuracy: 0.091837\n",
      "Validation:\n",
      "loss: 2.303354, metric: 0.091880\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 2.302718, batch accuracy: 0.081633\n",
      "Validation:\n",
      "loss: 2.303317, metric: 0.091880\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 2.302725, batch accuracy: 0.061224\n",
      "Validation:\n",
      "loss: 2.303259, metric: 0.091880\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 2.302686, batch accuracy: 0.102041\n",
      "Validation:\n",
      "loss: 2.303099, metric: 0.096154\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 2.302632, batch accuracy: 0.081633\n",
      "Validation:\n",
      "loss: 2.302790, metric: 0.104701\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 2.302691, batch accuracy: 0.122449\n",
      "Validation:\n",
      "loss: 2.303393, metric: 0.091880\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 2.302653, batch accuracy: 0.040816\n",
      "Validation:\n",
      "loss: 2.303398, metric: 0.096154\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 2.302624, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.303633, metric: 0.091880\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 2.302646, batch accuracy: 0.102041\n",
      "Validation:\n",
      "loss: 2.303209, metric: 0.097222\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 2.302588, batch accuracy: 0.122449\n",
      "Validation:\n",
      "loss: 2.303404, metric: 0.091880\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 2.302634, batch accuracy: 0.071429\n",
      "Validation:\n",
      "loss: 2.303123, metric: 0.097222\n",
      "\n",
      "Epoch 31\n",
      "Batch:    88/88, Avg_loss: 2.302650, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.303263, metric: 0.091880\n",
      "\n",
      "Epoch 32\n",
      "Batch:    88/88, Avg_loss: 2.302594, batch accuracy: 0.153061\n",
      "Validation:\n",
      "loss: 2.303163, metric: 0.097222\n",
      "\n",
      "Epoch 33\n",
      "Batch:    88/88, Avg_loss: 2.302615, batch accuracy: 0.030612\n",
      "Validation:\n",
      "loss: 2.303065, metric: 0.091880\n",
      "\n",
      "Epoch 34\n",
      "Batch:    88/88, Avg_loss: 2.302567, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.303182, metric: 0.096154\n",
      "\n",
      "Epoch 35\n",
      "Batch:    88/88, Avg_loss: 2.302611, batch accuracy: 0.051020\n",
      "Validation:\n",
      "loss: 2.303299, metric: 0.091880\n",
      "\n",
      "Epoch 36\n",
      "Batch:    88/88, Avg_loss: 2.302580, batch accuracy: 0.091837\n",
      "Validation:\n",
      "loss: 2.303177, metric: 0.091880\n",
      "\n",
      "Epoch 37\n",
      "Batch:    88/88, Avg_loss: 2.302557, batch accuracy: 0.122449\n",
      "Validation:\n",
      "loss: 2.303275, metric: 0.097222\n",
      "\n",
      "Epoch 38\n",
      "Batch:    88/88, Avg_loss: 2.302577, batch accuracy: 0.153061\n",
      "Validation:\n",
      "loss: 2.303476, metric: 0.091880\n",
      "\n",
      "Epoch 39\n",
      "Batch:    88/88, Avg_loss: 2.302496, batch accuracy: 0.091837\n",
      "Validation:\n",
      "loss: 2.303204, metric: 0.091880\n",
      "\n",
      "Epoch 40\n",
      "Batch:    88/88, Avg_loss: 2.302537, batch accuracy: 0.163265\n",
      "Validation:\n",
      "loss: 2.303457, metric: 0.091880\n",
      "\n",
      "Epoch 41\n",
      "Batch:    88/88, Avg_loss: 2.302519, batch accuracy: 0.122449\n",
      "Validation:\n",
      "loss: 2.303181, metric: 0.096154\n",
      "\n",
      "Epoch 42\n",
      "Batch:    88/88, Avg_loss: 2.302543, batch accuracy: 0.102041\n",
      "Validation:\n",
      "loss: 2.303259, metric: 0.095085\n",
      "\n",
      "Epoch 43\n",
      "Batch:    88/88, Avg_loss: 2.302517, batch accuracy: 0.061224\n",
      "Validation:\n",
      "loss: 2.303329, metric: 0.091880\n",
      "\n",
      "Epoch 44\n",
      "Batch:    88/88, Avg_loss: 2.302536, batch accuracy: 0.122449\n",
      "Validation:\n",
      "loss: 2.303332, metric: 0.091880\n",
      "\n",
      "Epoch 45\n",
      "Batch:    88/88, Avg_loss: 2.302534, batch accuracy: 0.091837\n",
      "Validation:\n",
      "loss: 2.303082, metric: 0.096154\n",
      "\n",
      "Epoch 46\n",
      "Batch:    88/88, Avg_loss: 2.302522, batch accuracy: 0.071429\n",
      "Validation:\n",
      "loss: 2.302950, metric: 0.097222\n",
      "\n",
      "Epoch 47\n",
      "Batch:    88/88, Avg_loss: 2.302489, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.303096, metric: 0.097222\n",
      "\n",
      "Epoch 48\n",
      "Batch:    88/88, Avg_loss: 2.302520, batch accuracy: 0.081633\n",
      "Validation:\n",
      "loss: 2.303214, metric: 0.097222\n",
      "\n",
      "Epoch 49\n",
      "Batch:    88/88, Avg_loss: 2.302423, batch accuracy: 0.091837\n",
      "Validation:\n",
      "loss: 2.303453, metric: 0.091880\n",
      "\n",
      "Epoch 50\n",
      "Batch:    88/88, Avg_loss: 2.302469, batch accuracy: 0.061224\n",
      "Validation:\n",
      "loss: 2.303094, metric: 0.096154\n",
      "\n",
      "Testing:\n",
      "loss: 2.303487, metric: 0.097437\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10] #[28*28, 1024, 512, 256, 100, 10]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model2 = Network141(layer_sizes).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model2.parameters(), lr = 0.5) # Tried with 0.1, 0.01, 0.001 - gave the same test results. \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model2, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model2, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model2, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model is Overparameterized resulting in no learning or zero accuracy! Let's experiment with the ReLU for the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Experimentation with ReLU for the Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_relu(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(Network_relu, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features=layer_sizes[0], out_features=layer_sizes[1])\n",
    "        self.L2 = nn.Linear(in_features=layer_sizes[1], out_features=layer_sizes[2])\n",
    "        self.L3 = nn.Linear(in_features=layer_sizes[2], out_features=layer_sizes[3])\n",
    "        self.L4 = nn.Linear(in_features=layer_sizes[3], out_features=layer_sizes[4])\n",
    "        self.out = nn.Linear(in_features=layer_sizes[4], out_features=layer_sizes[5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.L1.weight.dtype)\n",
    "        L1 = F.relu(self.L1(x))\n",
    "        L2 = F.relu(self.L2(L1))\n",
    "        L3 = F.relu(self.L3(L2))\n",
    "        L4 = F.relu(self.L4(L3))\n",
    "        out = F.softmax(self.out(L4), dim=1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model3 = Network_relu(layer_sizes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 1, 1000]         785,000\n",
      "            Linear-2               [-1, 1, 500]         500,500\n",
      "            Linear-3               [-1, 1, 250]         125,250\n",
      "            Linear-4               [-1, 1, 100]          25,100\n",
      "            Linear-5                [-1, 1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 1,436,860\n",
      "Trainable params: 1,436,860\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 5.48\n",
      "Estimated Total Size (MB): 5.50\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model3, input_size=(1, input_size), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 2.300461, batch accuracy: 0.214286\n",
      "Validation:\n",
      "loss: 2.296251, metric: 0.240919\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 2.158938, batch accuracy: 0.387755\n",
      "Validation:\n",
      "loss: 1.937823, metric: 0.485310\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.767216, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.702908, metric: 0.773771\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.653919, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.655903, metric: 0.818109\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.609705, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.642217, metric: 0.825588\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.589937, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.630708, metric: 0.833066\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.576553, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.630301, metric: 0.833333\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.564456, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.634018, metric: 0.830128\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.560149, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.621087, metric: 0.840011\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.556195, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.633614, metric: 0.830929\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.552559, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.624173, metric: 0.836806\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.548959, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.618550, metric: 0.843483\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.546445, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.619869, metric: 0.840545\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.544833, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.619842, metric: 0.841346\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.542029, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.621111, metric: 0.838675\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.541102, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.616670, metric: 0.845085\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.540468, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.614491, metric: 0.847489\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.538314, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.619011, metric: 0.842147\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.538891, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.618840, metric: 0.841613\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.537430, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.618604, metric: 0.840545\n",
      "\n",
      "Testing:\n",
      "loss: 1.597562, metric: 0.861185\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "optimizer = optim.SGD(model3.parameters(), lr = 0.5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model3, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model3, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model3, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the ReLU activation function in the hidden layers has proven to be the most effective, as it provides higher accuracy compared to other activation functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1: Experimentation with Nestrov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 2.302473, batch accuracy: 0.091837\n",
      "Validation:\n",
      "loss: 2.302063, metric: 0.129808\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 2.301885, batch accuracy: 0.234694\n",
      "Validation:\n",
      "loss: 2.301404, metric: 0.173611\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 2.301090, batch accuracy: 0.265306\n",
      "Validation:\n",
      "loss: 2.300469, metric: 0.240652\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 2.299903, batch accuracy: 0.346939\n",
      "Validation:\n",
      "loss: 2.298998, metric: 0.297009\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 2.297915, batch accuracy: 0.336735\n",
      "Validation:\n",
      "loss: 2.296377, metric: 0.342949\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 2.294043, batch accuracy: 0.408163\n",
      "Validation:\n",
      "loss: 2.290862, metric: 0.458600\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 2.284280, batch accuracy: 0.489796\n",
      "Validation:\n",
      "loss: 2.274701, metric: 0.366720\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 2.244543, batch accuracy: 0.255102\n",
      "Validation:\n",
      "loss: 2.209526, metric: 0.225160\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 2.149002, batch accuracy: 0.489796\n",
      "Validation:\n",
      "loss: 2.071437, metric: 0.435096\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.962018, batch accuracy: 0.571429\n",
      "Validation:\n",
      "loss: 1.891481, metric: 0.582799\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.811277, batch accuracy: 0.714286\n",
      "Validation:\n",
      "loss: 1.767744, metric: 0.714744\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.732278, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 1.706062, metric: 0.800748\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.662870, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.664774, metric: 0.823184\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.635274, batch accuracy: 0.795918\n",
      "Validation:\n",
      "loss: 1.656828, metric: 0.822917\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.619440, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.649719, metric: 0.829327\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.606366, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.646479, metric: 0.831998\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.596450, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.641180, metric: 0.835470\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.588071, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.640814, metric: 0.833600\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.581051, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.639941, metric: 0.831464\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.575147, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.636471, metric: 0.835737\n",
      "\n",
      "Testing:\n",
      "loss: 1.624159, metric: 0.845168\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model4 = Network_relu(layer_sizes).to(device)\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(model4.parameters(), lr = 0.01, momentum=momentum, nesterov=True)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model4, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model4, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model4, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the performance: When I raise the learning rate from 0.001 to 0.01, the test accuracy improves from 0.35 to 0.845168 with the epoch 20. Nonetheless, with a learning rate of 0.001, we can still achieve higher accuracy by training the model for additional epochs and when done for 200 epochs, it achieved the same test accuracy as 0.84383 as shown below! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 2.302663, batch accuracy: 0.091837\n",
      "Validation:\n",
      "loss: 2.302496, metric: 0.103632\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 2.302610, batch accuracy: 0.173469\n",
      "Validation:\n",
      "loss: 2.302445, metric: 0.106571\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 2.302557, batch accuracy: 0.153061\n",
      "Validation:\n",
      "loss: 2.302394, metric: 0.111378\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 2.302503, batch accuracy: 0.132653\n",
      "Validation:\n",
      "loss: 2.302342, metric: 0.115652\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 2.302454, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.302289, metric: 0.119124\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 2.302398, batch accuracy: 0.102041\n",
      "Validation:\n",
      "loss: 2.302236, metric: 0.121795\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 2.302340, batch accuracy: 0.173469\n",
      "Validation:\n",
      "loss: 2.302181, metric: 0.124733\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 2.302285, batch accuracy: 0.153061\n",
      "Validation:\n",
      "loss: 2.302125, metric: 0.128205\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 2.302228, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.302068, metric: 0.129808\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 2.302169, batch accuracy: 0.132653\n",
      "Validation:\n",
      "loss: 2.302009, metric: 0.131944\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 2.302108, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.301949, metric: 0.135684\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 2.302043, batch accuracy: 0.173469\n",
      "Validation:\n",
      "loss: 2.301886, metric: 0.137821\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 2.301987, batch accuracy: 0.081633\n",
      "Validation:\n",
      "loss: 2.301822, metric: 0.143162\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 2.301914, batch accuracy: 0.234694\n",
      "Validation:\n",
      "loss: 2.301756, metric: 0.147436\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 2.301848, batch accuracy: 0.071429\n",
      "Validation:\n",
      "loss: 2.301687, metric: 0.153312\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 2.301774, batch accuracy: 0.214286\n",
      "Validation:\n",
      "loss: 2.301616, metric: 0.158387\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 2.301706, batch accuracy: 0.132653\n",
      "Validation:\n",
      "loss: 2.301543, metric: 0.162393\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 2.301628, batch accuracy: 0.081633\n",
      "Validation:\n",
      "loss: 2.301467, metric: 0.169071\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 2.301547, batch accuracy: 0.204082\n",
      "Validation:\n",
      "loss: 2.301388, metric: 0.174947\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 2.301470, batch accuracy: 0.153061\n",
      "Validation:\n",
      "loss: 2.301308, metric: 0.180823\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 2.301384, batch accuracy: 0.173469\n",
      "Validation:\n",
      "loss: 2.301222, metric: 0.188835\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 2.301296, batch accuracy: 0.163265\n",
      "Validation:\n",
      "loss: 2.301134, metric: 0.196047\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 2.301205, batch accuracy: 0.214286\n",
      "Validation:\n",
      "loss: 2.301043, metric: 0.205662\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 2.301111, batch accuracy: 0.183673\n",
      "Validation:\n",
      "loss: 2.300948, metric: 0.212607\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 2.301009, batch accuracy: 0.224490\n",
      "Validation:\n",
      "loss: 2.300848, metric: 0.220353\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 2.300905, batch accuracy: 0.265306\n",
      "Validation:\n",
      "loss: 2.300745, metric: 0.228632\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 2.300801, batch accuracy: 0.224490\n",
      "Validation:\n",
      "loss: 2.300637, metric: 0.234509\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 2.300690, batch accuracy: 0.173469\n",
      "Validation:\n",
      "loss: 2.300525, metric: 0.238515\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 2.300572, batch accuracy: 0.204082\n",
      "Validation:\n",
      "loss: 2.300407, metric: 0.244925\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 2.300445, batch accuracy: 0.346939\n",
      "Validation:\n",
      "loss: 2.300283, metric: 0.249733\n",
      "\n",
      "Epoch 31\n",
      "Batch:    88/88, Avg_loss: 2.300317, batch accuracy: 0.285714\n",
      "Validation:\n",
      "loss: 2.300153, metric: 0.254808\n",
      "\n",
      "Epoch 32\n",
      "Batch:    88/88, Avg_loss: 2.300184, batch accuracy: 0.265306\n",
      "Validation:\n",
      "loss: 2.300016, metric: 0.260951\n",
      "\n",
      "Epoch 33\n",
      "Batch:    88/88, Avg_loss: 2.300040, batch accuracy: 0.244898\n",
      "Validation:\n",
      "loss: 2.299872, metric: 0.264423\n",
      "\n",
      "Epoch 34\n",
      "Batch:    88/88, Avg_loss: 2.299890, batch accuracy: 0.265306\n",
      "Validation:\n",
      "loss: 2.299720, metric: 0.270299\n",
      "\n",
      "Epoch 35\n",
      "Batch:    88/88, Avg_loss: 2.299728, batch accuracy: 0.326531\n",
      "Validation:\n",
      "loss: 2.299560, metric: 0.275641\n",
      "\n",
      "Epoch 36\n",
      "Batch:    88/88, Avg_loss: 2.299563, batch accuracy: 0.224490\n",
      "Validation:\n",
      "loss: 2.299392, metric: 0.283387\n",
      "\n",
      "Epoch 37\n",
      "Batch:    88/88, Avg_loss: 2.299386, batch accuracy: 0.306122\n",
      "Validation:\n",
      "loss: 2.299213, metric: 0.287660\n",
      "\n",
      "Epoch 38\n",
      "Batch:    88/88, Avg_loss: 2.299199, batch accuracy: 0.255102\n",
      "Validation:\n",
      "loss: 2.299024, metric: 0.293269\n",
      "\n",
      "Epoch 39\n",
      "Batch:    88/88, Avg_loss: 2.298996, batch accuracy: 0.438776\n",
      "Validation:\n",
      "loss: 2.298824, metric: 0.298344\n",
      "\n",
      "Epoch 40\n",
      "Batch:    88/88, Avg_loss: 2.298791, batch accuracy: 0.255102\n",
      "Validation:\n",
      "loss: 2.298611, metric: 0.304220\n",
      "\n",
      "Epoch 41\n",
      "Batch:    88/88, Avg_loss: 2.298564, batch accuracy: 0.377551\n",
      "Validation:\n",
      "loss: 2.298385, metric: 0.309829\n",
      "\n",
      "Epoch 42\n",
      "Batch:    88/88, Avg_loss: 2.298332, batch accuracy: 0.275510\n",
      "Validation:\n",
      "loss: 2.298144, metric: 0.314904\n",
      "\n",
      "Epoch 43\n",
      "Batch:    88/88, Avg_loss: 2.298073, batch accuracy: 0.346939\n",
      "Validation:\n",
      "loss: 2.297886, metric: 0.318376\n",
      "\n",
      "Epoch 44\n",
      "Batch:    88/88, Avg_loss: 2.297806, batch accuracy: 0.285714\n",
      "Validation:\n",
      "loss: 2.297612, metric: 0.322115\n",
      "\n",
      "Epoch 45\n",
      "Batch:    88/88, Avg_loss: 2.297515, batch accuracy: 0.285714\n",
      "Validation:\n",
      "loss: 2.297317, metric: 0.326656\n",
      "\n",
      "Epoch 46\n",
      "Batch:    88/88, Avg_loss: 2.297203, batch accuracy: 0.306122\n",
      "Validation:\n",
      "loss: 2.297001, metric: 0.329327\n",
      "\n",
      "Epoch 47\n",
      "Batch:    88/88, Avg_loss: 2.296866, batch accuracy: 0.244898\n",
      "Validation:\n",
      "loss: 2.296660, metric: 0.333066\n",
      "\n",
      "Epoch 48\n",
      "Batch:    88/88, Avg_loss: 2.296507, batch accuracy: 0.275510\n",
      "Validation:\n",
      "loss: 2.296293, metric: 0.341346\n",
      "\n",
      "Epoch 49\n",
      "Batch:    88/88, Avg_loss: 2.296120, batch accuracy: 0.285714\n",
      "Validation:\n",
      "loss: 2.295897, metric: 0.348024\n",
      "\n",
      "Epoch 50\n",
      "Batch:    88/88, Avg_loss: 2.295696, batch accuracy: 0.408163\n",
      "Validation:\n",
      "loss: 2.295469, metric: 0.352297\n",
      "\n",
      "Epoch 51\n",
      "Batch:    88/88, Avg_loss: 2.295237, batch accuracy: 0.377551\n",
      "Validation:\n",
      "loss: 2.295003, metric: 0.359776\n",
      "\n",
      "Epoch 52\n",
      "Batch:    88/88, Avg_loss: 2.294747, batch accuracy: 0.346939\n",
      "Validation:\n",
      "loss: 2.294495, metric: 0.367521\n",
      "\n",
      "Epoch 53\n",
      "Batch:    88/88, Avg_loss: 2.294206, batch accuracy: 0.295918\n",
      "Validation:\n",
      "loss: 2.293939, metric: 0.377404\n",
      "\n",
      "Epoch 54\n",
      "Batch:    88/88, Avg_loss: 2.293607, batch accuracy: 0.428571\n",
      "Validation:\n",
      "loss: 2.293334, metric: 0.390224\n",
      "\n",
      "Epoch 55\n",
      "Batch:    88/88, Avg_loss: 2.292959, batch accuracy: 0.397959\n",
      "Validation:\n",
      "loss: 2.292667, metric: 0.405449\n",
      "\n",
      "Epoch 56\n",
      "Batch:    88/88, Avg_loss: 2.292244, batch accuracy: 0.408163\n",
      "Validation:\n",
      "loss: 2.291932, metric: 0.440972\n",
      "\n",
      "Epoch 57\n",
      "Batch:    88/88, Avg_loss: 2.291451, batch accuracy: 0.438776\n",
      "Validation:\n",
      "loss: 2.291115, metric: 0.459402\n",
      "\n",
      "Epoch 58\n",
      "Batch:    88/88, Avg_loss: 2.290570, batch accuracy: 0.448980\n",
      "Validation:\n",
      "loss: 2.290209, metric: 0.475962\n",
      "\n",
      "Epoch 59\n",
      "Batch:    88/88, Avg_loss: 2.289592, batch accuracy: 0.489796\n",
      "Validation:\n",
      "loss: 2.289192, metric: 0.490118\n",
      "\n",
      "Epoch 60\n",
      "Batch:    88/88, Avg_loss: 2.288478, batch accuracy: 0.602041\n",
      "Validation:\n",
      "loss: 2.288057, metric: 0.501870\n",
      "\n",
      "Epoch 61\n",
      "Batch:    88/88, Avg_loss: 2.287235, batch accuracy: 0.510204\n",
      "Validation:\n",
      "loss: 2.286767, metric: 0.506944\n",
      "\n",
      "Epoch 62\n",
      "Batch:    88/88, Avg_loss: 2.285811, batch accuracy: 0.530612\n",
      "Validation:\n",
      "loss: 2.285298, metric: 0.507479\n",
      "\n",
      "Epoch 63\n",
      "Batch:    88/88, Avg_loss: 2.284186, batch accuracy: 0.510204\n",
      "Validation:\n",
      "loss: 2.283614, metric: 0.500534\n",
      "\n",
      "Epoch 64\n",
      "Batch:    88/88, Avg_loss: 2.282303, batch accuracy: 0.520408\n",
      "Validation:\n",
      "loss: 2.281654, metric: 0.476229\n",
      "\n",
      "Epoch 65\n",
      "Batch:    88/88, Avg_loss: 2.280103, batch accuracy: 0.438776\n",
      "Validation:\n",
      "loss: 2.279369, metric: 0.443643\n",
      "\n",
      "Epoch 66\n",
      "Batch:    88/88, Avg_loss: 2.277522, batch accuracy: 0.326531\n",
      "Validation:\n",
      "loss: 2.276667, metric: 0.390224\n",
      "\n",
      "Epoch 67\n",
      "Batch:    88/88, Avg_loss: 2.274420, batch accuracy: 0.357143\n",
      "Validation:\n",
      "loss: 2.273435, metric: 0.331731\n",
      "\n",
      "Epoch 68\n",
      "Batch:    88/88, Avg_loss: 2.270683, batch accuracy: 0.285714\n",
      "Validation:\n",
      "loss: 2.269487, metric: 0.291132\n",
      "\n",
      "Epoch 69\n",
      "Batch:    88/88, Avg_loss: 2.266090, batch accuracy: 0.255102\n",
      "Validation:\n",
      "loss: 2.264693, metric: 0.264690\n",
      "\n",
      "Epoch 70\n",
      "Batch:    88/88, Avg_loss: 2.260418, batch accuracy: 0.204082\n",
      "Validation:\n",
      "loss: 2.258741, metric: 0.243056\n",
      "\n",
      "Epoch 71\n",
      "Batch:    88/88, Avg_loss: 2.253343, batch accuracy: 0.244898\n",
      "Validation:\n",
      "loss: 2.251550, metric: 0.228632\n",
      "\n",
      "Epoch 72\n",
      "Batch:    88/88, Avg_loss: 2.244781, batch accuracy: 0.224490\n",
      "Validation:\n",
      "loss: 2.243056, metric: 0.222756\n",
      "\n",
      "Epoch 73\n",
      "Batch:    88/88, Avg_loss: 2.234830, batch accuracy: 0.224490\n",
      "Validation:\n",
      "loss: 2.233850, metric: 0.218750\n",
      "\n",
      "Epoch 74\n",
      "Batch:    88/88, Avg_loss: 2.224279, batch accuracy: 0.234694\n",
      "Validation:\n",
      "loss: 2.224707, metric: 0.218483\n",
      "\n",
      "Epoch 75\n",
      "Batch:    88/88, Avg_loss: 2.214290, batch accuracy: 0.204082\n",
      "Validation:\n",
      "loss: 2.216536, metric: 0.222222\n",
      "\n",
      "Epoch 76\n",
      "Batch:    88/88, Avg_loss: 2.205308, batch accuracy: 0.285714\n",
      "Validation:\n",
      "loss: 2.208879, metric: 0.225962\n",
      "\n",
      "Epoch 77\n",
      "Batch:    88/88, Avg_loss: 2.197018, batch accuracy: 0.255102\n",
      "Validation:\n",
      "loss: 2.200685, metric: 0.238782\n",
      "\n",
      "Epoch 78\n",
      "Batch:    88/88, Avg_loss: 2.188186, batch accuracy: 0.234694\n",
      "Validation:\n",
      "loss: 2.191494, metric: 0.258547\n",
      "\n",
      "Epoch 79\n",
      "Batch:    88/88, Avg_loss: 2.178204, batch accuracy: 0.244898\n",
      "Validation:\n",
      "loss: 2.180822, metric: 0.292735\n",
      "\n",
      "Epoch 80\n",
      "Batch:    88/88, Avg_loss: 2.166730, batch accuracy: 0.244898\n",
      "Validation:\n",
      "loss: 2.168476, metric: 0.316506\n",
      "\n",
      "Epoch 81\n",
      "Batch:    88/88, Avg_loss: 2.153371, batch accuracy: 0.295918\n",
      "Validation:\n",
      "loss: 2.154058, metric: 0.336806\n",
      "\n",
      "Epoch 82\n",
      "Batch:    88/88, Avg_loss: 2.137963, batch accuracy: 0.408163\n",
      "Validation:\n",
      "loss: 2.137911, metric: 0.356303\n",
      "\n",
      "Epoch 83\n",
      "Batch:    88/88, Avg_loss: 2.120774, batch accuracy: 0.438776\n",
      "Validation:\n",
      "loss: 2.119197, metric: 0.382479\n",
      "\n",
      "Epoch 84\n",
      "Batch:    88/88, Avg_loss: 2.101471, batch accuracy: 0.418367\n",
      "Validation:\n",
      "loss: 2.098202, metric: 0.402511\n",
      "\n",
      "Epoch 85\n",
      "Batch:    88/88, Avg_loss: 2.079355, batch accuracy: 0.346939\n",
      "Validation:\n",
      "loss: 2.073982, metric: 0.444177\n",
      "\n",
      "Epoch 86\n",
      "Batch:    88/88, Avg_loss: 2.053552, batch accuracy: 0.459184\n",
      "Validation:\n",
      "loss: 2.046333, metric: 0.478365\n",
      "\n",
      "Epoch 87\n",
      "Batch:    88/88, Avg_loss: 2.025481, batch accuracy: 0.561224\n",
      "Validation:\n",
      "loss: 2.017940, metric: 0.515491\n",
      "\n",
      "Epoch 88\n",
      "Batch:    88/88, Avg_loss: 1.998612, batch accuracy: 0.510204\n",
      "Validation:\n",
      "loss: 1.992421, metric: 0.528846\n",
      "\n",
      "Epoch 89\n",
      "Batch:    88/88, Avg_loss: 1.974791, batch accuracy: 0.520408\n",
      "Validation:\n",
      "loss: 1.971275, metric: 0.536058\n",
      "\n",
      "Epoch 90\n",
      "Batch:    88/88, Avg_loss: 1.954693, batch accuracy: 0.540816\n",
      "Validation:\n",
      "loss: 1.952861, metric: 0.543002\n",
      "\n",
      "Epoch 91\n",
      "Batch:    88/88, Avg_loss: 1.937107, batch accuracy: 0.642857\n",
      "Validation:\n",
      "loss: 1.937657, metric: 0.547009\n",
      "\n",
      "Epoch 92\n",
      "Batch:    88/88, Avg_loss: 1.921491, batch accuracy: 0.581633\n",
      "Validation:\n",
      "loss: 1.922608, metric: 0.564637\n",
      "\n",
      "Epoch 93\n",
      "Batch:    88/88, Avg_loss: 1.906881, batch accuracy: 0.571429\n",
      "Validation:\n",
      "loss: 1.908931, metric: 0.580395\n",
      "\n",
      "Epoch 94\n",
      "Batch:    88/88, Avg_loss: 1.892452, batch accuracy: 0.653061\n",
      "Validation:\n",
      "loss: 1.895027, metric: 0.594284\n",
      "\n",
      "Epoch 95\n",
      "Batch:    88/88, Avg_loss: 1.876726, batch accuracy: 0.632653\n",
      "Validation:\n",
      "loss: 1.878411, metric: 0.646368\n",
      "\n",
      "Epoch 96\n",
      "Batch:    88/88, Avg_loss: 1.858378, batch accuracy: 0.673469\n",
      "Validation:\n",
      "loss: 1.859499, metric: 0.671207\n",
      "\n",
      "Epoch 97\n",
      "Batch:    88/88, Avg_loss: 1.840060, batch accuracy: 0.663265\n",
      "Validation:\n",
      "loss: 1.841967, metric: 0.685630\n",
      "\n",
      "Epoch 98\n",
      "Batch:    88/88, Avg_loss: 1.821915, batch accuracy: 0.734694\n",
      "Validation:\n",
      "loss: 1.825575, metric: 0.697382\n",
      "\n",
      "Epoch 99\n",
      "Batch:    88/88, Avg_loss: 1.805877, batch accuracy: 0.704082\n",
      "Validation:\n",
      "loss: 1.811187, metric: 0.700588\n",
      "\n",
      "Epoch 100\n",
      "Batch:    88/88, Avg_loss: 1.791695, batch accuracy: 0.673469\n",
      "Validation:\n",
      "loss: 1.798510, metric: 0.704060\n",
      "\n",
      "Epoch 101\n",
      "Batch:    88/88, Avg_loss: 1.779808, batch accuracy: 0.622449\n",
      "Validation:\n",
      "loss: 1.788371, metric: 0.708600\n",
      "\n",
      "Epoch 102\n",
      "Batch:    88/88, Avg_loss: 1.769777, batch accuracy: 0.683673\n",
      "Validation:\n",
      "loss: 1.779618, metric: 0.712340\n",
      "\n",
      "Epoch 103\n",
      "Batch:    88/88, Avg_loss: 1.761270, batch accuracy: 0.724490\n",
      "Validation:\n",
      "loss: 1.772990, metric: 0.711806\n",
      "\n",
      "Epoch 104\n",
      "Batch:    88/88, Avg_loss: 1.754573, batch accuracy: 0.744898\n",
      "Validation:\n",
      "loss: 1.767367, metric: 0.714476\n",
      "\n",
      "Epoch 105\n",
      "Batch:    88/88, Avg_loss: 1.748798, batch accuracy: 0.704082\n",
      "Validation:\n",
      "loss: 1.762502, metric: 0.716880\n",
      "\n",
      "Epoch 106\n",
      "Batch:    88/88, Avg_loss: 1.743748, batch accuracy: 0.724490\n",
      "Validation:\n",
      "loss: 1.758303, metric: 0.720620\n",
      "\n",
      "Epoch 107\n",
      "Batch:    88/88, Avg_loss: 1.739592, batch accuracy: 0.663265\n",
      "Validation:\n",
      "loss: 1.754470, metric: 0.720887\n",
      "\n",
      "Epoch 108\n",
      "Batch:    88/88, Avg_loss: 1.735496, batch accuracy: 0.734694\n",
      "Validation:\n",
      "loss: 1.751520, metric: 0.721688\n",
      "\n",
      "Epoch 109\n",
      "Batch:    88/88, Avg_loss: 1.731878, batch accuracy: 0.714286\n",
      "Validation:\n",
      "loss: 1.748696, metric: 0.720887\n",
      "\n",
      "Epoch 110\n",
      "Batch:    88/88, Avg_loss: 1.728429, batch accuracy: 0.785714\n",
      "Validation:\n",
      "loss: 1.745788, metric: 0.723024\n",
      "\n",
      "Epoch 111\n",
      "Batch:    88/88, Avg_loss: 1.725001, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 1.743052, metric: 0.724893\n",
      "\n",
      "Epoch 112\n",
      "Batch:    88/88, Avg_loss: 1.720893, batch accuracy: 0.734694\n",
      "Validation:\n",
      "loss: 1.736535, metric: 0.727030\n",
      "\n",
      "Epoch 113\n",
      "Batch:    88/88, Avg_loss: 1.700316, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.706596, metric: 0.786058\n",
      "\n",
      "Epoch 114\n",
      "Batch:    88/88, Avg_loss: 1.683620, batch accuracy: 0.775510\n",
      "Validation:\n",
      "loss: 1.697756, metric: 0.789263\n",
      "\n",
      "Epoch 115\n",
      "Batch:    88/88, Avg_loss: 1.675395, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.691110, metric: 0.810363\n",
      "\n",
      "Epoch 116\n",
      "Batch:    88/88, Avg_loss: 1.669142, batch accuracy: 0.775510\n",
      "Validation:\n",
      "loss: 1.686393, metric: 0.809295\n",
      "\n",
      "Epoch 117\n",
      "Batch:    88/88, Avg_loss: 1.663475, batch accuracy: 0.785714\n",
      "Validation:\n",
      "loss: 1.682155, metric: 0.812767\n",
      "\n",
      "Epoch 118\n",
      "Batch:    88/88, Avg_loss: 1.658804, batch accuracy: 0.806122\n",
      "Validation:\n",
      "loss: 1.678513, metric: 0.815705\n",
      "\n",
      "Epoch 119\n",
      "Batch:    88/88, Avg_loss: 1.654460, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.675415, metric: 0.818109\n",
      "\n",
      "Epoch 120\n",
      "Batch:    88/88, Avg_loss: 1.650924, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.673605, metric: 0.818643\n",
      "\n",
      "Epoch 121\n",
      "Batch:    88/88, Avg_loss: 1.647787, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.671054, metric: 0.818643\n",
      "\n",
      "Epoch 122\n",
      "Batch:    88/88, Avg_loss: 1.644611, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.668820, metric: 0.820780\n",
      "\n",
      "Epoch 123\n",
      "Batch:    88/88, Avg_loss: 1.641935, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.666860, metric: 0.822650\n",
      "\n",
      "Epoch 124\n",
      "Batch:    88/88, Avg_loss: 1.639204, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.665664, metric: 0.821581\n",
      "\n",
      "Epoch 125\n",
      "Batch:    88/88, Avg_loss: 1.637045, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.664298, metric: 0.822650\n",
      "\n",
      "Epoch 126\n",
      "Batch:    88/88, Avg_loss: 1.634707, batch accuracy: 0.806122\n",
      "Validation:\n",
      "loss: 1.663177, metric: 0.822650\n",
      "\n",
      "Epoch 127\n",
      "Batch:    88/88, Avg_loss: 1.632240, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.662271, metric: 0.823985\n",
      "\n",
      "Epoch 128\n",
      "Batch:    88/88, Avg_loss: 1.630506, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.660896, metric: 0.822650\n",
      "\n",
      "Epoch 129\n",
      "Batch:    88/88, Avg_loss: 1.628596, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.660309, metric: 0.823184\n",
      "\n",
      "Epoch 130\n",
      "Batch:    88/88, Avg_loss: 1.626622, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.659378, metric: 0.824786\n",
      "\n",
      "Epoch 131\n",
      "Batch:    88/88, Avg_loss: 1.624720, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.658318, metric: 0.823718\n",
      "\n",
      "Epoch 132\n",
      "Batch:    88/88, Avg_loss: 1.622873, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.657496, metric: 0.823718\n",
      "\n",
      "Epoch 133\n",
      "Batch:    88/88, Avg_loss: 1.621314, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.656799, metric: 0.824519\n",
      "\n",
      "Epoch 134\n",
      "Batch:    88/88, Avg_loss: 1.619660, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.656624, metric: 0.823451\n",
      "\n",
      "Epoch 135\n",
      "Batch:    88/88, Avg_loss: 1.617974, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.655813, metric: 0.823184\n",
      "\n",
      "Epoch 136\n",
      "Batch:    88/88, Avg_loss: 1.616555, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.654886, metric: 0.825855\n",
      "\n",
      "Epoch 137\n",
      "Batch:    88/88, Avg_loss: 1.614881, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.654520, metric: 0.825855\n",
      "\n",
      "Epoch 138\n",
      "Batch:    88/88, Avg_loss: 1.613453, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.653841, metric: 0.826389\n",
      "\n",
      "Epoch 139\n",
      "Batch:    88/88, Avg_loss: 1.611948, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.653611, metric: 0.825321\n",
      "\n",
      "Epoch 140\n",
      "Batch:    88/88, Avg_loss: 1.610681, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.652972, metric: 0.825053\n",
      "\n",
      "Epoch 141\n",
      "Batch:    88/88, Avg_loss: 1.609397, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.652398, metric: 0.826389\n",
      "\n",
      "Epoch 142\n",
      "Batch:    88/88, Avg_loss: 1.608308, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.651919, metric: 0.826923\n",
      "\n",
      "Epoch 143\n",
      "Batch:    88/88, Avg_loss: 1.606976, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.651514, metric: 0.828259\n",
      "\n",
      "Epoch 144\n",
      "Batch:    88/88, Avg_loss: 1.605658, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.651083, metric: 0.826656\n",
      "\n",
      "Epoch 145\n",
      "Batch:    88/88, Avg_loss: 1.604398, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.650334, metric: 0.827190\n",
      "\n",
      "Epoch 146\n",
      "Batch:    88/88, Avg_loss: 1.603295, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.650395, metric: 0.827724\n",
      "\n",
      "Epoch 147\n",
      "Batch:    88/88, Avg_loss: 1.602207, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.649997, metric: 0.827457\n",
      "\n",
      "Epoch 148\n",
      "Batch:    88/88, Avg_loss: 1.601236, batch accuracy: 0.795918\n",
      "Validation:\n",
      "loss: 1.649365, metric: 0.827991\n",
      "\n",
      "Epoch 149\n",
      "Batch:    88/88, Avg_loss: 1.599927, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.648856, metric: 0.827457\n",
      "\n",
      "Epoch 150\n",
      "Batch:    88/88, Avg_loss: 1.598730, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.648519, metric: 0.829060\n",
      "\n",
      "Epoch 151\n",
      "Batch:    88/88, Avg_loss: 1.597856, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.648111, metric: 0.827724\n",
      "\n",
      "Epoch 152\n",
      "Batch:    88/88, Avg_loss: 1.596903, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.647943, metric: 0.827991\n",
      "\n",
      "Epoch 153\n",
      "Batch:    88/88, Avg_loss: 1.595829, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.647733, metric: 0.828526\n",
      "\n",
      "Epoch 154\n",
      "Batch:    88/88, Avg_loss: 1.594773, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.646875, metric: 0.829594\n",
      "\n",
      "Epoch 155\n",
      "Batch:    88/88, Avg_loss: 1.593951, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.646717, metric: 0.828526\n",
      "\n",
      "Epoch 156\n",
      "Batch:    88/88, Avg_loss: 1.593169, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 1.646651, metric: 0.828526\n",
      "\n",
      "Epoch 157\n",
      "Batch:    88/88, Avg_loss: 1.591989, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.645939, metric: 0.829327\n",
      "\n",
      "Epoch 158\n",
      "Batch:    88/88, Avg_loss: 1.591189, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.645555, metric: 0.829861\n",
      "\n",
      "Epoch 159\n",
      "Batch:    88/88, Avg_loss: 1.590088, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.645266, metric: 0.830128\n",
      "\n",
      "Epoch 160\n",
      "Batch:    88/88, Avg_loss: 1.589355, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.645126, metric: 0.829861\n",
      "\n",
      "Epoch 161\n",
      "Batch:    88/88, Avg_loss: 1.588554, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.644890, metric: 0.829861\n",
      "\n",
      "Epoch 162\n",
      "Batch:    88/88, Avg_loss: 1.587773, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.644481, metric: 0.831464\n",
      "\n",
      "Epoch 163\n",
      "Batch:    88/88, Avg_loss: 1.587008, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.644479, metric: 0.828793\n",
      "\n",
      "Epoch 164\n",
      "Batch:    88/88, Avg_loss: 1.586085, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.644127, metric: 0.830929\n",
      "\n",
      "Epoch 165\n",
      "Batch:    88/88, Avg_loss: 1.585227, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.643792, metric: 0.830929\n",
      "\n",
      "Epoch 166\n",
      "Batch:    88/88, Avg_loss: 1.584368, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.643946, metric: 0.830395\n",
      "\n",
      "Epoch 167\n",
      "Batch:    88/88, Avg_loss: 1.583778, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.643025, metric: 0.832265\n",
      "\n",
      "Epoch 168\n",
      "Batch:    88/88, Avg_loss: 1.582987, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.642980, metric: 0.831464\n",
      "\n",
      "Epoch 169\n",
      "Batch:    88/88, Avg_loss: 1.582192, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.643375, metric: 0.830395\n",
      "\n",
      "Epoch 170\n",
      "Batch:    88/88, Avg_loss: 1.581490, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.642576, metric: 0.832532\n",
      "\n",
      "Epoch 171\n",
      "Batch:    88/88, Avg_loss: 1.580884, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.642736, metric: 0.830929\n",
      "\n",
      "Epoch 172\n",
      "Batch:    88/88, Avg_loss: 1.580221, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.642346, metric: 0.830929\n",
      "\n",
      "Epoch 173\n",
      "Batch:    88/88, Avg_loss: 1.579635, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.641945, metric: 0.831998\n",
      "\n",
      "Epoch 174\n",
      "Batch:    88/88, Avg_loss: 1.579065, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.641914, metric: 0.832265\n",
      "\n",
      "Epoch 175\n",
      "Batch:    88/88, Avg_loss: 1.578285, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.642018, metric: 0.829861\n",
      "\n",
      "Epoch 176\n",
      "Batch:    88/88, Avg_loss: 1.577825, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.641703, metric: 0.830662\n",
      "\n",
      "Epoch 177\n",
      "Batch:    88/88, Avg_loss: 1.577147, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.641753, metric: 0.831464\n",
      "\n",
      "Epoch 178\n",
      "Batch:    88/88, Avg_loss: 1.576531, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.641392, metric: 0.830662\n",
      "\n",
      "Epoch 179\n",
      "Batch:    88/88, Avg_loss: 1.576141, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.641349, metric: 0.831197\n",
      "\n",
      "Epoch 180\n",
      "Batch:    88/88, Avg_loss: 1.575464, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.641093, metric: 0.831464\n",
      "\n",
      "Epoch 181\n",
      "Batch:    88/88, Avg_loss: 1.574899, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.641264, metric: 0.829861\n",
      "\n",
      "Epoch 182\n",
      "Batch:    88/88, Avg_loss: 1.574459, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.640821, metric: 0.830929\n",
      "\n",
      "Epoch 183\n",
      "Batch:    88/88, Avg_loss: 1.573926, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.640325, metric: 0.832799\n",
      "\n",
      "Epoch 184\n",
      "Batch:    88/88, Avg_loss: 1.573584, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.640575, metric: 0.830929\n",
      "\n",
      "Epoch 185\n",
      "Batch:    88/88, Avg_loss: 1.572963, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.639998, metric: 0.832265\n",
      "\n",
      "Epoch 186\n",
      "Batch:    88/88, Avg_loss: 1.572584, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.639938, metric: 0.831731\n",
      "\n",
      "Epoch 187\n",
      "Batch:    88/88, Avg_loss: 1.572098, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.639605, metric: 0.831998\n",
      "\n",
      "Epoch 188\n",
      "Batch:    88/88, Avg_loss: 1.571688, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.639674, metric: 0.831998\n",
      "\n",
      "Epoch 189\n",
      "Batch:    88/88, Avg_loss: 1.571198, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.639288, metric: 0.833066\n",
      "\n",
      "Epoch 190\n",
      "Batch:    88/88, Avg_loss: 1.570759, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.639399, metric: 0.830128\n",
      "\n",
      "Epoch 191\n",
      "Batch:    88/88, Avg_loss: 1.570233, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.639296, metric: 0.830662\n",
      "\n",
      "Epoch 192\n",
      "Batch:    88/88, Avg_loss: 1.569691, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.639617, metric: 0.830662\n",
      "\n",
      "Epoch 193\n",
      "Batch:    88/88, Avg_loss: 1.569412, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.638782, metric: 0.831731\n",
      "\n",
      "Epoch 194\n",
      "Batch:    88/88, Avg_loss: 1.568913, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.638744, metric: 0.831998\n",
      "\n",
      "Epoch 195\n",
      "Batch:    88/88, Avg_loss: 1.568454, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.638636, metric: 0.831197\n",
      "\n",
      "Epoch 196\n",
      "Batch:    88/88, Avg_loss: 1.568116, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.638299, metric: 0.831731\n",
      "\n",
      "Epoch 197\n",
      "Batch:    88/88, Avg_loss: 1.567536, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.637959, metric: 0.831464\n",
      "\n",
      "Epoch 198\n",
      "Batch:    88/88, Avg_loss: 1.567177, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.637867, metric: 0.831464\n",
      "\n",
      "Epoch 199\n",
      "Batch:    88/88, Avg_loss: 1.566826, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.637601, metric: 0.832265\n",
      "\n",
      "Epoch 200\n",
      "Batch:    88/88, Avg_loss: 1.566348, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.637348, metric: 0.832265\n",
      "\n",
      "Testing:\n",
      "loss: 1.624266, metric: 0.843833\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model4 = Network_relu(layer_sizes).to(device)\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(model4.parameters(), lr = 0.001, momentum=momentum, nesterov=True)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model4, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model4, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model4, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2: Experimentation with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 2.107163, batch accuracy: 0.653061\n",
      "Validation:\n",
      "loss: 1.804107, metric: 0.691506\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.690895, batch accuracy: 0.795918\n",
      "Validation:\n",
      "loss: 1.657764, metric: 0.812500\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.631485, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.643273, metric: 0.821047\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.610457, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.640285, metric: 0.841880\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.595164, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.635594, metric: 0.844818\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.583540, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.628597, metric: 0.848558\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.573976, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.624469, metric: 0.854701\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.566346, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.624127, metric: 0.854968\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.560112, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.621332, metric: 0.853365\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.554413, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.619872, metric: 0.855502\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.551051, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.617267, metric: 0.857372\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.547986, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.617460, metric: 0.854968\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.545245, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.616963, metric: 0.853900\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.542260, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.613187, metric: 0.854968\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.540027, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.609721, metric: 0.858974\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.537059, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.608875, metric: 0.856303\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.534275, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.609559, metric: 0.852297\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.533302, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.609392, metric: 0.854167\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.532101, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.605642, metric: 0.857105\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.530899, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.606041, metric: 0.857372\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.530049, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.606843, metric: 0.854434\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.529878, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.606240, metric: 0.856838\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.529116, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.604941, metric: 0.857105\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.529390, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.605249, metric: 0.857639\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.528461, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.606851, metric: 0.855235\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.528054, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.606426, metric: 0.855769\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.528027, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.609343, metric: 0.853098\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.527930, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.606837, metric: 0.852831\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.528181, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.604600, metric: 0.856303\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.526215, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.604343, metric: 0.856571\n",
      "\n",
      "Epoch 31\n",
      "Batch:    88/88, Avg_loss: 1.525302, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.604195, metric: 0.857639\n",
      "\n",
      "Epoch 32\n",
      "Batch:    88/88, Avg_loss: 1.524978, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.605216, metric: 0.856571\n",
      "\n",
      "Epoch 33\n",
      "Batch:    88/88, Avg_loss: 1.523928, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.604100, metric: 0.857372\n",
      "\n",
      "Epoch 34\n",
      "Batch:    88/88, Avg_loss: 1.524261, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.604048, metric: 0.858440\n",
      "\n",
      "Epoch 35\n",
      "Batch:    88/88, Avg_loss: 1.523656, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.605408, metric: 0.855769\n",
      "\n",
      "Epoch 36\n",
      "Batch:    88/88, Avg_loss: 1.523352, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.602787, metric: 0.858707\n",
      "\n",
      "Epoch 37\n",
      "Batch:    88/88, Avg_loss: 1.523357, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.604195, metric: 0.858707\n",
      "\n",
      "Epoch 38\n",
      "Batch:    88/88, Avg_loss: 1.522473, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.603640, metric: 0.858440\n",
      "\n",
      "Epoch 39\n",
      "Batch:    88/88, Avg_loss: 1.522033, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.604003, metric: 0.857372\n",
      "\n",
      "Epoch 40\n",
      "Batch:    88/88, Avg_loss: 1.521741, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.604356, metric: 0.856571\n",
      "\n",
      "Epoch 41\n",
      "Batch:    88/88, Avg_loss: 1.521740, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.605015, metric: 0.855769\n",
      "\n",
      "Epoch 42\n",
      "Batch:    88/88, Avg_loss: 1.522131, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.606085, metric: 0.855235\n",
      "\n",
      "Epoch 43\n",
      "Batch:    88/88, Avg_loss: 1.523692, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.606965, metric: 0.855235\n",
      "\n",
      "Epoch 44\n",
      "Batch:    88/88, Avg_loss: 1.523966, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.608412, metric: 0.854167\n",
      "\n",
      "Epoch 45\n",
      "Batch:    88/88, Avg_loss: 1.522901, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.604277, metric: 0.859241\n",
      "\n",
      "Epoch 46\n",
      "Batch:    88/88, Avg_loss: 1.521616, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.604469, metric: 0.858173\n",
      "\n",
      "Epoch 47\n",
      "Batch:    88/88, Avg_loss: 1.521865, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.610750, metric: 0.849893\n",
      "\n",
      "Epoch 48\n",
      "Batch:    88/88, Avg_loss: 1.521592, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.605328, metric: 0.855769\n",
      "\n",
      "Epoch 49\n",
      "Batch:    88/88, Avg_loss: 1.520818, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.606140, metric: 0.856838\n",
      "\n",
      "Epoch 50\n",
      "Batch:    88/88, Avg_loss: 1.520936, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.602853, metric: 0.857906\n",
      "\n",
      "Testing:\n",
      "loss: 1.594976, metric: 0.864389\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "model5 = Network_relu(layer_sizes).to(device)\n",
    "optimizer = optim.Adam(model5.parameters(), lr = 0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model5, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model5, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model5, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the highest accuracy has been achieved using the ADAM optimizer with a learning rate of 0.001, resulting in a test accuracy of 0.864.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Implementing Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network_dropout(nn.Module):\n",
    "    def __init__(self, layer_sizes, dropout_prob=0.5):\n",
    "        super(Network_dropout, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features=layer_sizes[0], out_features=layer_sizes[1])\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "        self.L2 = nn.Linear(in_features=layer_sizes[1], out_features=layer_sizes[2])\n",
    "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "        self.L3 = nn.Linear(in_features=layer_sizes[2], out_features=layer_sizes[3])\n",
    "        self.dropout3 = nn.Dropout(p=dropout_prob)\n",
    "        self.L4 = nn.Linear(in_features=layer_sizes[3], out_features=layer_sizes[4])\n",
    "        self.dropout4 = nn.Dropout(p=dropout_prob)\n",
    "        self.out = nn.Linear(in_features=layer_sizes[4], out_features=layer_sizes[5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.L1.weight.dtype)\n",
    "        L1 = F.relu(self.L1(x))\n",
    "        L1 = self.dropout1(L1)\n",
    "        L2 = F.relu(self.L2(L1))\n",
    "        L2 = self.dropout2(L2)\n",
    "        L3 = F.relu(self.L3(L2))\n",
    "        L3 = self.dropout3(L3)\n",
    "        L4 = F.relu(self.L4(L3))\n",
    "        L4 = self.dropout4(L4)\n",
    "        out = F.softmax(self.out(L4), dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 1, 1000]         785,000\n",
      "           Dropout-2              [-1, 1, 1000]               0\n",
      "            Linear-3               [-1, 1, 500]         500,500\n",
      "           Dropout-4               [-1, 1, 500]               0\n",
      "            Linear-5               [-1, 1, 250]         125,250\n",
      "           Dropout-6               [-1, 1, 250]               0\n",
      "            Linear-7               [-1, 1, 100]          25,100\n",
      "           Dropout-8               [-1, 1, 100]               0\n",
      "            Linear-9                [-1, 1, 10]           1,010\n",
      "================================================================\n",
      "Total params: 1,436,860\n",
      "Trainable params: 1,436,860\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 5.48\n",
      "Estimated Total Size (MB): 5.51\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 2.294114, batch accuracy: 0.234694\n",
      "Validation:\n",
      "loss: 2.253965, metric: 0.274306\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 2.094834, batch accuracy: 0.510204\n",
      "Validation:\n",
      "loss: 1.950348, metric: 0.540064\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.874812, batch accuracy: 0.581633\n",
      "Validation:\n",
      "loss: 1.834973, metric: 0.639957\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.793700, batch accuracy: 0.775510\n",
      "Validation:\n",
      "loss: 1.781448, metric: 0.696848\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.735529, batch accuracy: 0.795918\n",
      "Validation:\n",
      "loss: 1.723826, metric: 0.759882\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.688165, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.694951, metric: 0.785524\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.662594, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.678262, metric: 0.805021\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.648200, batch accuracy: 0.785714\n",
      "Validation:\n",
      "loss: 1.674342, metric: 0.802618\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.641663, batch accuracy: 0.775510\n",
      "Validation:\n",
      "loss: 1.659453, metric: 0.821047\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.630519, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.656614, metric: 0.823451\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.623410, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.653563, metric: 0.826656\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.617487, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.654591, metric: 0.822917\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.611789, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.645946, metric: 0.831998\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.605621, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.644287, metric: 0.836538\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.600579, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.641127, metric: 0.834669\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.594617, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.639760, metric: 0.836806\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.593799, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.639439, metric: 0.837874\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.587639, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.637890, metric: 0.837607\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.584452, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.631356, metric: 0.843483\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.582249, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.632008, metric: 0.841346\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.580458, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.632835, metric: 0.841346\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.576692, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.634146, metric: 0.842147\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.572260, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.630754, metric: 0.840011\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.571782, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.624741, metric: 0.846688\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.568937, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.625910, metric: 0.844284\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.564650, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.623412, metric: 0.848558\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.561402, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.625442, metric: 0.841079\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.560417, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.620705, metric: 0.846154\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.561122, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.622148, metric: 0.844818\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.557593, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.624621, metric: 0.839744\n",
      "\n",
      "Epoch 31\n",
      "Batch:    88/88, Avg_loss: 1.555278, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.625945, metric: 0.840545\n",
      "\n",
      "Epoch 32\n",
      "Batch:    88/88, Avg_loss: 1.555410, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.618763, metric: 0.844551\n",
      "\n",
      "Epoch 33\n",
      "Batch:    88/88, Avg_loss: 1.552307, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.619113, metric: 0.844818\n",
      "\n",
      "Epoch 34\n",
      "Batch:    88/88, Avg_loss: 1.551843, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.617934, metric: 0.845085\n",
      "\n",
      "Epoch 35\n",
      "Batch:    88/88, Avg_loss: 1.549741, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.620519, metric: 0.843216\n",
      "\n",
      "Epoch 36\n",
      "Batch:    88/88, Avg_loss: 1.548218, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.613532, metric: 0.851763\n",
      "\n",
      "Epoch 37\n",
      "Batch:    88/88, Avg_loss: 1.546927, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.622705, metric: 0.839744\n",
      "\n",
      "Epoch 38\n",
      "Batch:    88/88, Avg_loss: 1.545968, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.620680, metric: 0.841346\n",
      "\n",
      "Epoch 39\n",
      "Batch:    88/88, Avg_loss: 1.545723, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.612945, metric: 0.851496\n",
      "\n",
      "Epoch 40\n",
      "Batch:    88/88, Avg_loss: 1.543408, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.612115, metric: 0.852564\n",
      "\n",
      "Epoch 41\n",
      "Batch:    88/88, Avg_loss: 1.543151, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.616089, metric: 0.848558\n",
      "\n",
      "Epoch 42\n",
      "Batch:    88/88, Avg_loss: 1.541156, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.612023, metric: 0.851496\n",
      "\n",
      "Epoch 43\n",
      "Batch:    88/88, Avg_loss: 1.540998, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.614191, metric: 0.848825\n",
      "\n",
      "Epoch 44\n",
      "Batch:    88/88, Avg_loss: 1.540375, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.608330, metric: 0.854167\n",
      "\n",
      "Epoch 45\n",
      "Batch:    88/88, Avg_loss: 1.539783, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.611571, metric: 0.851763\n",
      "\n",
      "Epoch 46\n",
      "Batch:    88/88, Avg_loss: 1.538728, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.608690, metric: 0.852831\n",
      "\n",
      "Epoch 47\n",
      "Batch:    88/88, Avg_loss: 1.538535, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.609125, metric: 0.854434\n",
      "\n",
      "Epoch 48\n",
      "Batch:    88/88, Avg_loss: 1.537291, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.614237, metric: 0.849359\n",
      "\n",
      "Epoch 49\n",
      "Batch:    88/88, Avg_loss: 1.537669, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.613500, metric: 0.850427\n",
      "\n",
      "Epoch 50\n",
      "Batch:    88/88, Avg_loss: 1.536184, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.616886, metric: 0.844017\n",
      "\n",
      "Testing:\n",
      "loss: 1.596564, metric: 0.865190\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "model6 = Network_dropout(layer_sizes).to(device)\n",
    "optimizer = optim.Adam(model6.parameters(), lr = 0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(model6, input_size=(1, input_size), device=device.type)\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model6, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model6, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model6, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of dropout barely improved in this case. Perhaps using a different batch size for the dataset could enhance the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Implementing Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_batchnorm(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(Network_batchnorm, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features=layer_sizes[0], out_features=layer_sizes[1])\n",
    "        self.bn1 = nn.BatchNorm1d(layer_sizes[1])\n",
    "        self.L2 = nn.Linear(in_features=layer_sizes[1], out_features=layer_sizes[2])\n",
    "        self.bn2 = nn.BatchNorm1d(layer_sizes[2])\n",
    "        self.L3 = nn.Linear(in_features=layer_sizes[2], out_features=layer_sizes[3])\n",
    "        self.bn3 = nn.BatchNorm1d(layer_sizes[3])\n",
    "        self.L4 = nn.Linear(in_features=layer_sizes[3], out_features=layer_sizes[4])\n",
    "        self.bn4 = nn.BatchNorm1d(layer_sizes[4])\n",
    "        self.out = nn.Linear(in_features=layer_sizes[4], out_features=layer_sizes[5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.L1.weight.dtype)\n",
    "        L1 = F.relu(self.bn1(self.L1(x)))\n",
    "        L2 = F.relu(self.bn2(self.L2(L1)))\n",
    "        L3 = F.relu(self.bn3(self.L3(L2)))\n",
    "        L4 = F.relu(self.bn4(self.L4(L3)))\n",
    "        out = F.softmax(self.out(L4), dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 2.019475, batch accuracy: 0.724490\n",
      "Validation:\n",
      "loss: 1.853032, metric: 0.811165\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.779722, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.746288, metric: 0.842415\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.693249, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.697710, metric: 0.850694\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.642014, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.670465, metric: 0.856571\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.603418, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.656244, metric: 0.859509\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.573324, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.645980, metric: 0.859509\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.551928, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.641536, metric: 0.858974\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.536795, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.636454, metric: 0.859509\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.524945, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.633551, metric: 0.858707\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.513777, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.631464, metric: 0.859776\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.506832, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.628670, metric: 0.857906\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.501098, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.627960, metric: 0.857105\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.497927, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.625946, metric: 0.858707\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.494820, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.623689, metric: 0.861645\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.491652, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.624154, metric: 0.859241\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.489190, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622692, metric: 0.857372\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.486998, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.623650, metric: 0.857906\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.485732, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.621397, metric: 0.856036\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.485000, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.620676, metric: 0.856036\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.483450, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.620382, metric: 0.855769\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.481790, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.619936, metric: 0.855502\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.481285, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.620925, metric: 0.854701\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.480671, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618163, metric: 0.856571\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.480106, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.620110, metric: 0.854701\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.479532, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.619136, metric: 0.854701\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.478879, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.615769, metric: 0.858440\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.478886, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.617855, metric: 0.854701\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.478505, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.617439, metric: 0.852564\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.478154, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.615716, metric: 0.855769\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.477620, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.613431, metric: 0.857906\n",
      "\n",
      "Epoch 31\n",
      "Batch:    88/88, Avg_loss: 1.478034, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.616372, metric: 0.853900\n",
      "\n",
      "Epoch 32\n",
      "Batch:    88/88, Avg_loss: 1.477710, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.613619, metric: 0.854968\n",
      "\n",
      "Epoch 33\n",
      "Batch:    88/88, Avg_loss: 1.480242, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618307, metric: 0.851763\n",
      "\n",
      "Epoch 34\n",
      "Batch:    88/88, Avg_loss: 1.480543, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.623480, metric: 0.846154\n",
      "\n",
      "Epoch 35\n",
      "Batch:    88/88, Avg_loss: 1.481306, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618561, metric: 0.851763\n",
      "\n",
      "Epoch 36\n",
      "Batch:    88/88, Avg_loss: 1.479365, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.619808, metric: 0.851763\n",
      "\n",
      "Epoch 37\n",
      "Batch:    88/88, Avg_loss: 1.479709, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.616018, metric: 0.852297\n",
      "\n",
      "Epoch 38\n",
      "Batch:    88/88, Avg_loss: 1.478785, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.617393, metric: 0.851763\n",
      "\n",
      "Epoch 39\n",
      "Batch:    88/88, Avg_loss: 1.479377, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.618246, metric: 0.847489\n",
      "\n",
      "Epoch 40\n",
      "Batch:    88/88, Avg_loss: 1.478367, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.616319, metric: 0.852297\n",
      "\n",
      "Epoch 41\n",
      "Batch:    88/88, Avg_loss: 1.477603, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.615098, metric: 0.851763\n",
      "\n",
      "Epoch 42\n",
      "Batch:    88/88, Avg_loss: 1.476938, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.613302, metric: 0.857105\n",
      "\n",
      "Epoch 43\n",
      "Batch:    88/88, Avg_loss: 1.476915, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.614131, metric: 0.852030\n",
      "\n",
      "Epoch 44\n",
      "Batch:    88/88, Avg_loss: 1.477059, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615121, metric: 0.853098\n",
      "\n",
      "Epoch 45\n",
      "Batch:    88/88, Avg_loss: 1.477195, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.610308, metric: 0.856838\n",
      "\n",
      "Epoch 46\n",
      "Batch:    88/88, Avg_loss: 1.476995, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.611551, metric: 0.854701\n",
      "\n",
      "Epoch 47\n",
      "Batch:    88/88, Avg_loss: 1.476883, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.610926, metric: 0.857639\n",
      "\n",
      "Epoch 48\n",
      "Batch:    88/88, Avg_loss: 1.476596, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.612136, metric: 0.857372\n",
      "\n",
      "Epoch 49\n",
      "Batch:    88/88, Avg_loss: 1.476342, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.606820, metric: 0.859776\n",
      "\n",
      "Epoch 50\n",
      "Batch:    88/88, Avg_loss: 1.476390, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.607946, metric: 0.859509\n",
      "\n",
      "Testing:\n",
      "loss: 1.590232, metric: 0.873198\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "model7 = Network_batchnorm(layer_sizes).to(device)\n",
    "optimizer = optim.Adam(model7.parameters(), lr = 0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model7, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model7, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model7, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accuracy that we got up until now is from batch normalization that is 0.873"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Relu Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_leakyrelu(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(Network_leakyrelu, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features=layer_sizes[0], out_features=layer_sizes[1])\n",
    "        self.bn1 = nn.BatchNorm1d(layer_sizes[1])\n",
    "        self.L2 = nn.Linear(in_features=layer_sizes[1], out_features=layer_sizes[2])\n",
    "        self.bn2 = nn.BatchNorm1d(layer_sizes[2])\n",
    "        self.L3 = nn.Linear(in_features=layer_sizes[2], out_features=layer_sizes[3])\n",
    "        self.bn3 = nn.BatchNorm1d(layer_sizes[3])\n",
    "        self.L4 = nn.Linear(in_features=layer_sizes[3], out_features=layer_sizes[4])\n",
    "        self.bn4 = nn.BatchNorm1d(layer_sizes[4])\n",
    "        self.out = nn.Linear(in_features=layer_sizes[4], out_features=layer_sizes[5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.L1.weight.dtype)\n",
    "        L1 = F.leaky_relu(self.bn1(self.L1(x)))\n",
    "        L2 = F.leaky_relu(self.bn2(self.L2(L1)))\n",
    "        L3 = F.leaky_relu(self.bn3(self.L3(L2)))\n",
    "        L4 = F.leaky_relu(self.bn4(self.L4(L3)))\n",
    "        out = F.softmax(self.out(L4), dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 2.015140, batch accuracy: 0.724490\n",
      "Validation:\n",
      "loss: 1.849782, metric: 0.810897\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.777767, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.745121, metric: 0.842415\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.692331, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.697112, metric: 0.849893\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.641513, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.670214, metric: 0.855769\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.603238, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.656239, metric: 0.859241\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.573153, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.645919, metric: 0.860043\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.551768, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.641550, metric: 0.858173\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.536759, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.636735, metric: 0.861378\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.524746, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.633366, metric: 0.859776\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.514067, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.631472, metric: 0.859509\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.507090, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.628596, metric: 0.856838\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.501160, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.628184, metric: 0.855235\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.497851, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.627015, metric: 0.857906\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.494668, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.624501, metric: 0.857906\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.491268, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.622998, metric: 0.859509\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.488893, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.623169, metric: 0.857372\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.487403, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.623649, metric: 0.857105\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.485829, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.624852, metric: 0.853098\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.484346, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.622866, metric: 0.854701\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.483000, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.622743, metric: 0.853365\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.481425, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.620625, metric: 0.855769\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.480554, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.622272, metric: 0.852831\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.479564, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618036, metric: 0.857906\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.479906, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.619725, metric: 0.853098\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.480150, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.620067, metric: 0.855502\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.479782, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.621924, metric: 0.848291\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.479744, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.619030, metric: 0.854167\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.479112, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.614561, metric: 0.857639\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.478625, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618607, metric: 0.854434\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.478306, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.616056, metric: 0.855235\n",
      "\n",
      "Epoch 31\n",
      "Batch:    88/88, Avg_loss: 1.478332, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618143, metric: 0.854701\n",
      "\n",
      "Epoch 32\n",
      "Batch:    88/88, Avg_loss: 1.477239, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615401, metric: 0.856036\n",
      "\n",
      "Epoch 33\n",
      "Batch:    88/88, Avg_loss: 1.477225, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615788, metric: 0.853632\n",
      "\n",
      "Epoch 34\n",
      "Batch:    88/88, Avg_loss: 1.478077, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.619744, metric: 0.850160\n",
      "\n",
      "Epoch 35\n",
      "Batch:    88/88, Avg_loss: 1.478122, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.619333, metric: 0.849359\n",
      "\n",
      "Epoch 36\n",
      "Batch:    88/88, Avg_loss: 1.476937, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.614681, metric: 0.854434\n",
      "\n",
      "Epoch 37\n",
      "Batch:    88/88, Avg_loss: 1.476847, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.615124, metric: 0.854434\n",
      "\n",
      "Epoch 38\n",
      "Batch:    88/88, Avg_loss: 1.479685, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.622249, metric: 0.849626\n",
      "\n",
      "Epoch 39\n",
      "Batch:    88/88, Avg_loss: 1.478828, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.620969, metric: 0.845887\n",
      "\n",
      "Epoch 40\n",
      "Batch:    88/88, Avg_loss: 1.478771, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.619424, metric: 0.849626\n",
      "\n",
      "Epoch 41\n",
      "Batch:    88/88, Avg_loss: 1.480027, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.624666, metric: 0.842415\n",
      "\n",
      "Epoch 42\n",
      "Batch:    88/88, Avg_loss: 1.479592, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.620909, metric: 0.845887\n",
      "\n",
      "Epoch 43\n",
      "Batch:    88/88, Avg_loss: 1.479537, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.623403, metric: 0.844017\n",
      "\n",
      "Epoch 44\n",
      "Batch:    88/88, Avg_loss: 1.479445, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.617754, metric: 0.849626\n",
      "\n",
      "Epoch 45\n",
      "Batch:    88/88, Avg_loss: 1.478853, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.616854, metric: 0.849092\n",
      "\n",
      "Epoch 46\n",
      "Batch:    88/88, Avg_loss: 1.478246, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.617253, metric: 0.848024\n",
      "\n",
      "Epoch 47\n",
      "Batch:    88/88, Avg_loss: 1.477180, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.616975, metric: 0.849893\n",
      "\n",
      "Epoch 48\n",
      "Batch:    88/88, Avg_loss: 1.477536, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.618176, metric: 0.845620\n",
      "\n",
      "Epoch 49\n",
      "Batch:    88/88, Avg_loss: 1.476783, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.614723, metric: 0.851763\n",
      "\n",
      "Epoch 50\n",
      "Batch:    88/88, Avg_loss: 1.476531, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.613708, metric: 0.851496\n",
      "\n",
      "Epoch 51\n",
      "Batch:    88/88, Avg_loss: 1.476157, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.612886, metric: 0.851496\n",
      "\n",
      "Epoch 52\n",
      "Batch:    88/88, Avg_loss: 1.476244, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.611480, metric: 0.852564\n",
      "\n",
      "Epoch 53\n",
      "Batch:    88/88, Avg_loss: 1.475953, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.612583, metric: 0.853900\n",
      "\n",
      "Epoch 54\n",
      "Batch:    88/88, Avg_loss: 1.475992, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.612218, metric: 0.851496\n",
      "\n",
      "Epoch 55\n",
      "Batch:    88/88, Avg_loss: 1.475694, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.611796, metric: 0.853632\n",
      "\n",
      "Epoch 56\n",
      "Batch:    88/88, Avg_loss: 1.475692, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.611490, metric: 0.853632\n",
      "\n",
      "Epoch 57\n",
      "Batch:    88/88, Avg_loss: 1.475637, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.614362, metric: 0.850160\n",
      "\n",
      "Epoch 58\n",
      "Batch:    88/88, Avg_loss: 1.475985, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.614377, metric: 0.851496\n",
      "\n",
      "Epoch 59\n",
      "Batch:    88/88, Avg_loss: 1.476477, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.614968, metric: 0.849893\n",
      "\n",
      "Epoch 60\n",
      "Batch:    88/88, Avg_loss: 1.475696, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615037, metric: 0.850160\n",
      "\n",
      "Epoch 61\n",
      "Batch:    88/88, Avg_loss: 1.477524, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.618152, metric: 0.848291\n",
      "\n",
      "Epoch 62\n",
      "Batch:    88/88, Avg_loss: 1.478225, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.617081, metric: 0.848291\n",
      "\n",
      "Epoch 63\n",
      "Batch:    88/88, Avg_loss: 1.476532, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.612915, metric: 0.851496\n",
      "\n",
      "Epoch 64\n",
      "Batch:    88/88, Avg_loss: 1.477142, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.616050, metric: 0.846421\n",
      "\n",
      "Epoch 65\n",
      "Batch:    88/88, Avg_loss: 1.477880, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.618739, metric: 0.845353\n",
      "\n",
      "Epoch 66\n",
      "Batch:    88/88, Avg_loss: 1.478769, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.617460, metric: 0.845620\n",
      "\n",
      "Epoch 67\n",
      "Batch:    88/88, Avg_loss: 1.478554, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.613062, metric: 0.850694\n",
      "\n",
      "Epoch 68\n",
      "Batch:    88/88, Avg_loss: 1.477424, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.613162, metric: 0.850962\n",
      "\n",
      "Epoch 69\n",
      "Batch:    88/88, Avg_loss: 1.476929, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.615381, metric: 0.850427\n",
      "\n",
      "Epoch 70\n",
      "Batch:    88/88, Avg_loss: 1.476563, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.611844, metric: 0.852030\n",
      "\n",
      "Epoch 71\n",
      "Batch:    88/88, Avg_loss: 1.476477, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.611319, metric: 0.852831\n",
      "\n",
      "Epoch 72\n",
      "Batch:    88/88, Avg_loss: 1.476821, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.611196, metric: 0.852564\n",
      "\n",
      "Epoch 73\n",
      "Batch:    88/88, Avg_loss: 1.476134, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.612640, metric: 0.852030\n",
      "\n",
      "Epoch 74\n",
      "Batch:    88/88, Avg_loss: 1.475909, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.612650, metric: 0.849893\n",
      "\n",
      "Epoch 75\n",
      "Batch:    88/88, Avg_loss: 1.476309, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.611090, metric: 0.854167\n",
      "\n",
      "Epoch 76\n",
      "Batch:    88/88, Avg_loss: 1.475836, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.611272, metric: 0.851763\n",
      "\n",
      "Epoch 77\n",
      "Batch:    88/88, Avg_loss: 1.476112, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.612177, metric: 0.851229\n",
      "\n",
      "Epoch 78\n",
      "Batch:    88/88, Avg_loss: 1.476368, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.616079, metric: 0.849092\n",
      "\n",
      "Epoch 79\n",
      "Batch:    88/88, Avg_loss: 1.476865, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.613270, metric: 0.849092\n",
      "\n",
      "Epoch 80\n",
      "Batch:    88/88, Avg_loss: 1.476519, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.613799, metric: 0.849626\n",
      "\n",
      "Epoch 81\n",
      "Batch:    88/88, Avg_loss: 1.476936, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.615432, metric: 0.848291\n",
      "\n",
      "Epoch 82\n",
      "Batch:    88/88, Avg_loss: 1.476849, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.619224, metric: 0.842147\n",
      "\n",
      "Epoch 83\n",
      "Batch:    88/88, Avg_loss: 1.477238, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.613711, metric: 0.850694\n",
      "\n",
      "Epoch 84\n",
      "Batch:    88/88, Avg_loss: 1.477133, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.611067, metric: 0.852030\n",
      "\n",
      "Epoch 85\n",
      "Batch:    88/88, Avg_loss: 1.476446, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.613489, metric: 0.850427\n",
      "\n",
      "Epoch 86\n",
      "Batch:    88/88, Avg_loss: 1.477185, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.614162, metric: 0.849092\n",
      "\n",
      "Epoch 87\n",
      "Batch:    88/88, Avg_loss: 1.477274, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.619528, metric: 0.847222\n",
      "\n",
      "Epoch 88\n",
      "Batch:    88/88, Avg_loss: 1.477218, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.614810, metric: 0.849092\n",
      "\n",
      "Epoch 89\n",
      "Batch:    88/88, Avg_loss: 1.476853, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.614161, metric: 0.851229\n",
      "\n",
      "Epoch 90\n",
      "Batch:    88/88, Avg_loss: 1.476755, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.613872, metric: 0.851763\n",
      "\n",
      "Epoch 91\n",
      "Batch:    88/88, Avg_loss: 1.477001, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.613328, metric: 0.850962\n",
      "\n",
      "Epoch 92\n",
      "Batch:    88/88, Avg_loss: 1.476724, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.611419, metric: 0.852831\n",
      "\n",
      "Epoch 93\n",
      "Batch:    88/88, Avg_loss: 1.476803, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.614092, metric: 0.849893\n",
      "\n",
      "Epoch 94\n",
      "Batch:    88/88, Avg_loss: 1.476237, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.613210, metric: 0.850694\n",
      "\n",
      "Epoch 95\n",
      "Batch:    88/88, Avg_loss: 1.475979, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.614955, metric: 0.849626\n",
      "\n",
      "Epoch 96\n",
      "Batch:    88/88, Avg_loss: 1.475810, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.614142, metric: 0.848024\n",
      "\n",
      "Epoch 97\n",
      "Batch:    88/88, Avg_loss: 1.475730, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.612804, metric: 0.849893\n",
      "\n",
      "Epoch 98\n",
      "Batch:    88/88, Avg_loss: 1.475855, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.611071, metric: 0.853900\n",
      "\n",
      "Epoch 99\n",
      "Batch:    88/88, Avg_loss: 1.475766, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.610622, metric: 0.852564\n",
      "\n",
      "Epoch 100\n",
      "Batch:    88/88, Avg_loss: 1.475602, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.610362, metric: 0.853098\n",
      "\n",
      "Testing:\n",
      "loss: 1.592313, metric: 0.868660\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "model8 = Network_leakyrelu(layer_sizes).to(device)\n",
    "optimizer = optim.Adam(model8.parameters(), lr = 0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model8, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model8, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model8, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Exponential ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_exprelu(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(Network_exprelu, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features=layer_sizes[0], out_features=layer_sizes[1])\n",
    "        self.bn1 = nn.BatchNorm1d(layer_sizes[1])\n",
    "        self.L2 = nn.Linear(in_features=layer_sizes[1], out_features=layer_sizes[2])\n",
    "        self.bn2 = nn.BatchNorm1d(layer_sizes[2])\n",
    "        self.L3 = nn.Linear(in_features=layer_sizes[2], out_features=layer_sizes[3])\n",
    "        self.bn3 = nn.BatchNorm1d(layer_sizes[3])\n",
    "        self.L4 = nn.Linear(in_features=layer_sizes[3], out_features=layer_sizes[4])\n",
    "        self.bn4 = nn.BatchNorm1d(layer_sizes[4])\n",
    "        self.out = nn.Linear(in_features=layer_sizes[4], out_features=layer_sizes[5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.L1.weight.dtype)\n",
    "        L1 = F.elu(self.bn1(self.L1(x)))\n",
    "        L2 = F.elu(self.bn2(self.L2(L1)))\n",
    "        L3 = F.elu(self.bn3(self.L3(L2)))\n",
    "        L4 = F.elu(self.bn4(self.L4(L3)))\n",
    "        out = F.softmax(self.out(L4), dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.873280, batch accuracy: 0.775510\n",
      "Validation:\n",
      "loss: 1.752287, metric: 0.826923\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.707236, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.697811, metric: 0.841613\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.659504, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.672827, metric: 0.845620\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.630255, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.659294, metric: 0.851496\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.607150, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.651565, metric: 0.852564\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.586095, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.641762, metric: 0.853098\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.571453, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.640150, metric: 0.850160\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.560029, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.635564, metric: 0.854167\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.549511, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.633219, metric: 0.852030\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.539682, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.632028, metric: 0.853098\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.532528, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.630365, metric: 0.850160\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.525447, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.630933, metric: 0.853632\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.520973, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.628605, metric: 0.848825\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.515936, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.629058, metric: 0.848558\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.511973, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.625907, metric: 0.855769\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.508744, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.626496, metric: 0.850962\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.506805, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.626187, metric: 0.848825\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.503708, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.626526, metric: 0.847489\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.502196, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.627756, metric: 0.843483\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.500516, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.625664, metric: 0.849092\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.499063, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.625690, metric: 0.849359\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.498682, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.627514, metric: 0.847222\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.498504, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.625483, metric: 0.848291\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.496747, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.626419, metric: 0.845887\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.494393, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.625290, metric: 0.848825\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.493941, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.625855, metric: 0.847756\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.492540, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622757, metric: 0.848024\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.492511, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.625009, metric: 0.849893\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.492026, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.624788, metric: 0.845620\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.490451, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.625215, metric: 0.844284\n",
      "\n",
      "Epoch 31\n",
      "Batch:    88/88, Avg_loss: 1.489390, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.620696, metric: 0.847222\n",
      "\n",
      "Epoch 32\n",
      "Batch:    88/88, Avg_loss: 1.488238, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.623089, metric: 0.847489\n",
      "\n",
      "Epoch 33\n",
      "Batch:    88/88, Avg_loss: 1.489133, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.623475, metric: 0.846154\n",
      "\n",
      "Epoch 34\n",
      "Batch:    88/88, Avg_loss: 1.488608, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.618746, metric: 0.851763\n",
      "\n",
      "Epoch 35\n",
      "Batch:    88/88, Avg_loss: 1.488824, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622308, metric: 0.848291\n",
      "\n",
      "Epoch 36\n",
      "Batch:    88/88, Avg_loss: 1.488130, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.619961, metric: 0.850694\n",
      "\n",
      "Epoch 37\n",
      "Batch:    88/88, Avg_loss: 1.486984, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.620844, metric: 0.847489\n",
      "\n",
      "Epoch 38\n",
      "Batch:    88/88, Avg_loss: 1.486584, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.625681, metric: 0.844818\n",
      "\n",
      "Epoch 39\n",
      "Batch:    88/88, Avg_loss: 1.489901, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.623566, metric: 0.845085\n",
      "\n",
      "Epoch 40\n",
      "Batch:    88/88, Avg_loss: 1.487772, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.623312, metric: 0.842147\n",
      "\n",
      "Epoch 41\n",
      "Batch:    88/88, Avg_loss: 1.487130, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.623428, metric: 0.846421\n",
      "\n",
      "Epoch 42\n",
      "Batch:    88/88, Avg_loss: 1.486078, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.621117, metric: 0.849092\n",
      "\n",
      "Epoch 43\n",
      "Batch:    88/88, Avg_loss: 1.486398, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618576, metric: 0.851763\n",
      "\n",
      "Epoch 44\n",
      "Batch:    88/88, Avg_loss: 1.484973, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615318, metric: 0.853900\n",
      "\n",
      "Epoch 45\n",
      "Batch:    88/88, Avg_loss: 1.484689, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.617745, metric: 0.851229\n",
      "\n",
      "Epoch 46\n",
      "Batch:    88/88, Avg_loss: 1.483546, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.613859, metric: 0.855235\n",
      "\n",
      "Epoch 47\n",
      "Batch:    88/88, Avg_loss: 1.483692, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.615631, metric: 0.851763\n",
      "\n",
      "Epoch 48\n",
      "Batch:    88/88, Avg_loss: 1.483675, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.615080, metric: 0.853900\n",
      "\n",
      "Epoch 49\n",
      "Batch:    88/88, Avg_loss: 1.484861, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.617684, metric: 0.850160\n",
      "\n",
      "Epoch 50\n",
      "Batch:    88/88, Avg_loss: 1.485436, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.618835, metric: 0.848024\n",
      "\n",
      "Epoch 51\n",
      "Batch:    88/88, Avg_loss: 1.485187, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.620887, metric: 0.842682\n",
      "\n",
      "Epoch 52\n",
      "Batch:    88/88, Avg_loss: 1.484221, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.616959, metric: 0.850160\n",
      "\n",
      "Epoch 53\n",
      "Batch:    88/88, Avg_loss: 1.484025, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.619980, metric: 0.844017\n",
      "\n",
      "Epoch 54\n",
      "Batch:    88/88, Avg_loss: 1.486086, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.619691, metric: 0.847222\n",
      "\n",
      "Epoch 55\n",
      "Batch:    88/88, Avg_loss: 1.485574, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.622055, metric: 0.846421\n",
      "\n",
      "Epoch 56\n",
      "Batch:    88/88, Avg_loss: 1.485534, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.623650, metric: 0.842949\n",
      "\n",
      "Epoch 57\n",
      "Batch:    88/88, Avg_loss: 1.487497, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.624668, metric: 0.841346\n",
      "\n",
      "Epoch 58\n",
      "Batch:    88/88, Avg_loss: 1.485477, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.620619, metric: 0.846421\n",
      "\n",
      "Epoch 59\n",
      "Batch:    88/88, Avg_loss: 1.484360, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.620713, metric: 0.843750\n",
      "\n",
      "Epoch 60\n",
      "Batch:    88/88, Avg_loss: 1.484042, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618249, metric: 0.845620\n",
      "\n",
      "Epoch 61\n",
      "Batch:    88/88, Avg_loss: 1.482660, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.616333, metric: 0.846154\n",
      "\n",
      "Epoch 62\n",
      "Batch:    88/88, Avg_loss: 1.482471, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.616413, metric: 0.845353\n",
      "\n",
      "Epoch 63\n",
      "Batch:    88/88, Avg_loss: 1.482205, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.612683, metric: 0.852030\n",
      "\n",
      "Epoch 64\n",
      "Batch:    88/88, Avg_loss: 1.482210, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.613493, metric: 0.852564\n",
      "\n",
      "Epoch 65\n",
      "Batch:    88/88, Avg_loss: 1.483830, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.619378, metric: 0.848024\n",
      "\n",
      "Epoch 66\n",
      "Batch:    88/88, Avg_loss: 1.485736, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.623212, metric: 0.840011\n",
      "\n",
      "Epoch 67\n",
      "Batch:    88/88, Avg_loss: 1.485822, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.625951, metric: 0.837073\n",
      "\n",
      "Epoch 68\n",
      "Batch:    88/88, Avg_loss: 1.484810, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.621800, metric: 0.844818\n",
      "\n",
      "Epoch 69\n",
      "Batch:    88/88, Avg_loss: 1.483020, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.617657, metric: 0.847222\n",
      "\n",
      "Epoch 70\n",
      "Batch:    88/88, Avg_loss: 1.482038, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.614729, metric: 0.852564\n",
      "\n",
      "Epoch 71\n",
      "Batch:    88/88, Avg_loss: 1.481643, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.617165, metric: 0.849626\n",
      "\n",
      "Epoch 72\n",
      "Batch:    88/88, Avg_loss: 1.484311, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.620585, metric: 0.845085\n",
      "\n",
      "Epoch 73\n",
      "Batch:    88/88, Avg_loss: 1.483703, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622714, metric: 0.843483\n",
      "\n",
      "Epoch 74\n",
      "Batch:    88/88, Avg_loss: 1.482739, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.618088, metric: 0.848825\n",
      "\n",
      "Epoch 75\n",
      "Batch:    88/88, Avg_loss: 1.483432, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.618763, metric: 0.846421\n",
      "\n",
      "Epoch 76\n",
      "Batch:    88/88, Avg_loss: 1.482214, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615122, metric: 0.851763\n",
      "\n",
      "Epoch 77\n",
      "Batch:    88/88, Avg_loss: 1.482573, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.616197, metric: 0.848558\n",
      "\n",
      "Epoch 78\n",
      "Batch:    88/88, Avg_loss: 1.483472, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.620271, metric: 0.845085\n",
      "\n",
      "Epoch 79\n",
      "Batch:    88/88, Avg_loss: 1.483812, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.621667, metric: 0.844818\n",
      "\n",
      "Epoch 80\n",
      "Batch:    88/88, Avg_loss: 1.484077, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.619431, metric: 0.846421\n",
      "\n",
      "Epoch 81\n",
      "Batch:    88/88, Avg_loss: 1.482694, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.619299, metric: 0.844284\n",
      "\n",
      "Epoch 82\n",
      "Batch:    88/88, Avg_loss: 1.482371, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618041, metric: 0.847489\n",
      "\n",
      "Epoch 83\n",
      "Batch:    88/88, Avg_loss: 1.481716, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615626, metric: 0.848558\n",
      "\n",
      "Epoch 84\n",
      "Batch:    88/88, Avg_loss: 1.481438, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.616841, metric: 0.846688\n",
      "\n",
      "Epoch 85\n",
      "Batch:    88/88, Avg_loss: 1.481537, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.611377, metric: 0.852564\n",
      "\n",
      "Epoch 86\n",
      "Batch:    88/88, Avg_loss: 1.481075, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.610768, metric: 0.854167\n",
      "\n",
      "Epoch 87\n",
      "Batch:    88/88, Avg_loss: 1.480871, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.610752, metric: 0.853632\n",
      "\n",
      "Epoch 88\n",
      "Batch:    88/88, Avg_loss: 1.480909, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.609407, metric: 0.854968\n",
      "\n",
      "Epoch 89\n",
      "Batch:    88/88, Avg_loss: 1.480939, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.614387, metric: 0.849359\n",
      "\n",
      "Epoch 90\n",
      "Batch:    88/88, Avg_loss: 1.481926, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.623044, metric: 0.842147\n",
      "\n",
      "Epoch 91\n",
      "Batch:    88/88, Avg_loss: 1.485680, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.618785, metric: 0.846421\n",
      "\n",
      "Epoch 92\n",
      "Batch:    88/88, Avg_loss: 1.485611, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.617995, metric: 0.845620\n",
      "\n",
      "Epoch 93\n",
      "Batch:    88/88, Avg_loss: 1.483649, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.618334, metric: 0.847489\n",
      "\n",
      "Epoch 94\n",
      "Batch:    88/88, Avg_loss: 1.482157, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.617525, metric: 0.848291\n",
      "\n",
      "Epoch 95\n",
      "Batch:    88/88, Avg_loss: 1.481319, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.616214, metric: 0.846688\n",
      "\n",
      "Epoch 96\n",
      "Batch:    88/88, Avg_loss: 1.482018, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.614650, metric: 0.846154\n",
      "\n",
      "Epoch 97\n",
      "Batch:    88/88, Avg_loss: 1.481711, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.612915, metric: 0.849359\n",
      "\n",
      "Epoch 98\n",
      "Batch:    88/88, Avg_loss: 1.481395, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.613265, metric: 0.851496\n",
      "\n",
      "Epoch 99\n",
      "Batch:    88/88, Avg_loss: 1.480978, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.611445, metric: 0.852030\n",
      "\n",
      "Epoch 100\n",
      "Batch:    88/88, Avg_loss: 1.481220, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.611289, metric: 0.848825\n",
      "\n",
      "Testing:\n",
      "loss: 1.596563, metric: 0.865990\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "model9 = Network_exprelu(layer_sizes).to(device)\n",
    "optimizer = optim.Adam(model9.parameters(), lr = 0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model9, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model9, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model9, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Parametric ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_prelu(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(Network_prelu, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features=layer_sizes[0], out_features=layer_sizes[1])\n",
    "        self.bn1 = nn.BatchNorm1d(layer_sizes[1])\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        \n",
    "        self.L2 = nn.Linear(in_features=layer_sizes[1], out_features=layer_sizes[2])\n",
    "        self.bn2 = nn.BatchNorm1d(layer_sizes[2])\n",
    "        self.prelu2 = nn.PReLU()\n",
    "        \n",
    "        self.L3 = nn.Linear(in_features=layer_sizes[2], out_features=layer_sizes[3])\n",
    "        self.bn3 = nn.BatchNorm1d(layer_sizes[3])\n",
    "        self.prelu3 = nn.PReLU()\n",
    "        \n",
    "        self.L4 = nn.Linear(in_features=layer_sizes[3], out_features=layer_sizes[4])\n",
    "        self.bn4 = nn.BatchNorm1d(layer_sizes[4])\n",
    "        self.prelu4 = nn.PReLU()\n",
    "        \n",
    "        self.out = nn.Linear(in_features=layer_sizes[4], out_features=layer_sizes[5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.L1.weight.dtype)\n",
    "        L1 = self.prelu1(self.bn1(self.L1(x)))\n",
    "        L2 = self.prelu2(self.bn2(self.L2(L1)))\n",
    "        L3 = self.prelu3(self.bn3(self.L3(L2)))\n",
    "        L4 = self.prelu4(self.bn4(self.L4(L3)))\n",
    "        out = F.softmax(self.out(L4), dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.935730, batch accuracy: 0.744898\n",
      "Validation:\n",
      "loss: 1.797427, metric: 0.821314\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.742964, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.721932, metric: 0.841079\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.676881, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.685942, metric: 0.848291\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.635752, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.664925, metric: 0.855769\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.602872, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.653069, metric: 0.857372\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.575062, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.643063, metric: 0.858974\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.555488, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.640505, metric: 0.855235\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.541574, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.636148, metric: 0.857105\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.529749, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.633029, metric: 0.854434\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.518965, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.630992, metric: 0.856571\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.510867, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.629071, metric: 0.852297\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.504800, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.629715, metric: 0.854434\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.501110, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.628248, metric: 0.853900\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.497801, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.626341, metric: 0.852831\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.494913, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.624565, metric: 0.853365\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.492599, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622457, metric: 0.857105\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.490539, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.621931, metric: 0.856838\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.487804, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622451, metric: 0.853900\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.486861, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622934, metric: 0.851763\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.485021, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.620742, metric: 0.850962\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.484051, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.620961, metric: 0.852831\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.483389, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.623391, metric: 0.848558\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.482916, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622083, metric: 0.852831\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.481780, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.620462, metric: 0.853365\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.480445, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.619285, metric: 0.852564\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.480034, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.618197, metric: 0.853365\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.479876, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.617339, metric: 0.855769\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.479157, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.617249, metric: 0.854701\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.478264, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.616477, metric: 0.857372\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.478611, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615860, metric: 0.855769\n",
      "\n",
      "Epoch 31\n",
      "Batch:    88/88, Avg_loss: 1.478731, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.621212, metric: 0.849893\n",
      "\n",
      "Epoch 32\n",
      "Batch:    88/88, Avg_loss: 1.480250, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622319, metric: 0.849359\n",
      "\n",
      "Epoch 33\n",
      "Batch:    88/88, Avg_loss: 1.479346, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.617009, metric: 0.850962\n",
      "\n",
      "Epoch 34\n",
      "Batch:    88/88, Avg_loss: 1.478467, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.617202, metric: 0.853632\n",
      "\n",
      "Epoch 35\n",
      "Batch:    88/88, Avg_loss: 1.479395, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.620022, metric: 0.847489\n",
      "\n",
      "Epoch 36\n",
      "Batch:    88/88, Avg_loss: 1.479746, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.618446, metric: 0.848825\n",
      "\n",
      "Epoch 37\n",
      "Batch:    88/88, Avg_loss: 1.478642, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615460, metric: 0.854167\n",
      "\n",
      "Epoch 38\n",
      "Batch:    88/88, Avg_loss: 1.478692, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.619743, metric: 0.850694\n",
      "\n",
      "Epoch 39\n",
      "Batch:    88/88, Avg_loss: 1.477699, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.617417, metric: 0.850694\n",
      "\n",
      "Epoch 40\n",
      "Batch:    88/88, Avg_loss: 1.477693, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.617342, metric: 0.849893\n",
      "\n",
      "Epoch 41\n",
      "Batch:    88/88, Avg_loss: 1.476980, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.616612, metric: 0.850427\n",
      "\n",
      "Epoch 42\n",
      "Batch:    88/88, Avg_loss: 1.477420, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.615198, metric: 0.852831\n",
      "\n",
      "Epoch 43\n",
      "Batch:    88/88, Avg_loss: 1.476707, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.615277, metric: 0.850694\n",
      "\n",
      "Epoch 44\n",
      "Batch:    88/88, Avg_loss: 1.476913, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.612035, metric: 0.855502\n",
      "\n",
      "Epoch 45\n",
      "Batch:    88/88, Avg_loss: 1.477002, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.613830, metric: 0.854434\n",
      "\n",
      "Epoch 46\n",
      "Batch:    88/88, Avg_loss: 1.477419, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.613368, metric: 0.854434\n",
      "\n",
      "Epoch 47\n",
      "Batch:    88/88, Avg_loss: 1.477360, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.614278, metric: 0.852297\n",
      "\n",
      "Epoch 48\n",
      "Batch:    88/88, Avg_loss: 1.478523, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.620950, metric: 0.845887\n",
      "\n",
      "Epoch 49\n",
      "Batch:    88/88, Avg_loss: 1.479711, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.620803, metric: 0.845887\n",
      "\n",
      "Epoch 50\n",
      "Batch:    88/88, Avg_loss: 1.479204, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.613743, metric: 0.852831\n",
      "\n",
      "Testing:\n",
      "loss: 1.600194, metric: 0.865723\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "model10 = Network_prelu(layer_sizes).to(device)\n",
    "optimizer = optim.Adam(model10.parameters(), lr = 0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model10, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model10, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model10, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Exponential Linear Unit (ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network_exprelu(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(Network_exprelu, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features=layer_sizes[0], out_features=layer_sizes[1])\n",
    "        self.bn1 = nn.BatchNorm1d(layer_sizes[1])\n",
    "        self.L2 = nn.Linear(in_features=layer_sizes[1], out_features=layer_sizes[2])\n",
    "        self.bn2 = nn.BatchNorm1d(layer_sizes[2])\n",
    "        self.L3 = nn.Linear(in_features=layer_sizes[2], out_features=layer_sizes[3])\n",
    "        self.bn3 = nn.BatchNorm1d(layer_sizes[3])\n",
    "        self.L4 = nn.Linear(in_features=layer_sizes[3], out_features=layer_sizes[4])\n",
    "        self.bn4 = nn.BatchNorm1d(layer_sizes[4])\n",
    "        self.out = nn.Linear(in_features=layer_sizes[4], out_features=layer_sizes[5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.L1.weight.dtype)\n",
    "        \n",
    "        L1 = F.elu(self.bn1(self.L1(x)))\n",
    "        L2 = F.elu(self.bn2(self.L2(L1)))\n",
    "        L3 = F.elu(self.bn3(self.L3(L2)))\n",
    "        L4 = F.elu(self.bn4(self.L4(L3)))\n",
    "        \n",
    "        out = F.softmax(self.out(L4), dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.873280, batch accuracy: 0.775510\n",
      "Validation:\n",
      "loss: 1.752287, metric: 0.826923\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.707236, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.697811, metric: 0.841613\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.659504, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.672827, metric: 0.845620\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.630255, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.659294, metric: 0.851496\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.607150, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.651565, metric: 0.852564\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.586095, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.641762, metric: 0.853098\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.571453, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.640150, metric: 0.850160\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.560029, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.635564, metric: 0.854167\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.549511, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.633219, metric: 0.852030\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.539682, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.632028, metric: 0.853098\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.532528, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.630365, metric: 0.850160\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.525447, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.630933, metric: 0.853632\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.520973, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.628605, metric: 0.848825\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.515936, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.629058, metric: 0.848558\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.511973, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.625907, metric: 0.855769\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.508744, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.626496, metric: 0.850962\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.506805, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.626187, metric: 0.848825\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.503708, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.626526, metric: 0.847489\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.502196, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.627756, metric: 0.843483\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.500516, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.625664, metric: 0.849092\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.499063, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.625690, metric: 0.849359\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.498682, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.627514, metric: 0.847222\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.498504, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.625483, metric: 0.848291\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.496747, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.626419, metric: 0.845887\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.494393, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.625290, metric: 0.848825\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.493941, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.625855, metric: 0.847756\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.492540, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622757, metric: 0.848024\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.492511, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.625009, metric: 0.849893\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.492026, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.624788, metric: 0.845620\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.490451, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.625215, metric: 0.844284\n",
      "\n",
      "Epoch 31\n",
      "Batch:    88/88, Avg_loss: 1.489390, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.620696, metric: 0.847222\n",
      "\n",
      "Epoch 32\n",
      "Batch:    88/88, Avg_loss: 1.488238, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.623089, metric: 0.847489\n",
      "\n",
      "Epoch 33\n",
      "Batch:    88/88, Avg_loss: 1.489133, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.623475, metric: 0.846154\n",
      "\n",
      "Epoch 34\n",
      "Batch:    88/88, Avg_loss: 1.488608, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.618746, metric: 0.851763\n",
      "\n",
      "Epoch 35\n",
      "Batch:    88/88, Avg_loss: 1.488824, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622308, metric: 0.848291\n",
      "\n",
      "Epoch 36\n",
      "Batch:    88/88, Avg_loss: 1.488130, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.619961, metric: 0.850694\n",
      "\n",
      "Epoch 37\n",
      "Batch:    88/88, Avg_loss: 1.486984, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.620844, metric: 0.847489\n",
      "\n",
      "Epoch 38\n",
      "Batch:    88/88, Avg_loss: 1.486584, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.625681, metric: 0.844818\n",
      "\n",
      "Epoch 39\n",
      "Batch:    88/88, Avg_loss: 1.489901, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.623566, metric: 0.845085\n",
      "\n",
      "Epoch 40\n",
      "Batch:    88/88, Avg_loss: 1.487772, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.623312, metric: 0.842147\n",
      "\n",
      "Epoch 41\n",
      "Batch:    88/88, Avg_loss: 1.487130, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.623428, metric: 0.846421\n",
      "\n",
      "Epoch 42\n",
      "Batch:    88/88, Avg_loss: 1.486078, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.621117, metric: 0.849092\n",
      "\n",
      "Epoch 43\n",
      "Batch:    88/88, Avg_loss: 1.486398, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618576, metric: 0.851763\n",
      "\n",
      "Epoch 44\n",
      "Batch:    88/88, Avg_loss: 1.484973, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615318, metric: 0.853900\n",
      "\n",
      "Epoch 45\n",
      "Batch:    88/88, Avg_loss: 1.484689, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.617745, metric: 0.851229\n",
      "\n",
      "Epoch 46\n",
      "Batch:    88/88, Avg_loss: 1.483546, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.613859, metric: 0.855235\n",
      "\n",
      "Epoch 47\n",
      "Batch:    88/88, Avg_loss: 1.483692, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.615631, metric: 0.851763\n",
      "\n",
      "Epoch 48\n",
      "Batch:    88/88, Avg_loss: 1.483675, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.615080, metric: 0.853900\n",
      "\n",
      "Epoch 49\n",
      "Batch:    88/88, Avg_loss: 1.484861, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.617684, metric: 0.850160\n",
      "\n",
      "Epoch 50\n",
      "Batch:    88/88, Avg_loss: 1.485436, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.618835, metric: 0.848024\n",
      "\n",
      "Epoch 51\n",
      "Batch:    88/88, Avg_loss: 1.485187, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.620887, metric: 0.842682\n",
      "\n",
      "Epoch 52\n",
      "Batch:    88/88, Avg_loss: 1.484221, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.616959, metric: 0.850160\n",
      "\n",
      "Epoch 53\n",
      "Batch:    88/88, Avg_loss: 1.484025, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.619980, metric: 0.844017\n",
      "\n",
      "Epoch 54\n",
      "Batch:    88/88, Avg_loss: 1.486086, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.619691, metric: 0.847222\n",
      "\n",
      "Epoch 55\n",
      "Batch:    88/88, Avg_loss: 1.485574, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.622055, metric: 0.846421\n",
      "\n",
      "Epoch 56\n",
      "Batch:    88/88, Avg_loss: 1.485534, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.623650, metric: 0.842949\n",
      "\n",
      "Epoch 57\n",
      "Batch:    88/88, Avg_loss: 1.487497, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.624668, metric: 0.841346\n",
      "\n",
      "Epoch 58\n",
      "Batch:    88/88, Avg_loss: 1.485477, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.620619, metric: 0.846421\n",
      "\n",
      "Epoch 59\n",
      "Batch:    88/88, Avg_loss: 1.484360, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.620713, metric: 0.843750\n",
      "\n",
      "Epoch 60\n",
      "Batch:    88/88, Avg_loss: 1.484042, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618249, metric: 0.845620\n",
      "\n",
      "Epoch 61\n",
      "Batch:    88/88, Avg_loss: 1.482660, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.616333, metric: 0.846154\n",
      "\n",
      "Epoch 62\n",
      "Batch:    88/88, Avg_loss: 1.482471, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.616413, metric: 0.845353\n",
      "\n",
      "Epoch 63\n",
      "Batch:    88/88, Avg_loss: 1.482205, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.612683, metric: 0.852030\n",
      "\n",
      "Epoch 64\n",
      "Batch:    88/88, Avg_loss: 1.482210, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.613493, metric: 0.852564\n",
      "\n",
      "Epoch 65\n",
      "Batch:    88/88, Avg_loss: 1.483830, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.619378, metric: 0.848024\n",
      "\n",
      "Epoch 66\n",
      "Batch:    88/88, Avg_loss: 1.485736, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.623212, metric: 0.840011\n",
      "\n",
      "Epoch 67\n",
      "Batch:    88/88, Avg_loss: 1.485822, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.625951, metric: 0.837073\n",
      "\n",
      "Epoch 68\n",
      "Batch:    88/88, Avg_loss: 1.484810, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.621800, metric: 0.844818\n",
      "\n",
      "Epoch 69\n",
      "Batch:    88/88, Avg_loss: 1.483020, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.617657, metric: 0.847222\n",
      "\n",
      "Epoch 70\n",
      "Batch:    88/88, Avg_loss: 1.482038, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.614729, metric: 0.852564\n",
      "\n",
      "Epoch 71\n",
      "Batch:    88/88, Avg_loss: 1.481643, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.617165, metric: 0.849626\n",
      "\n",
      "Epoch 72\n",
      "Batch:    88/88, Avg_loss: 1.484311, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.620585, metric: 0.845085\n",
      "\n",
      "Epoch 73\n",
      "Batch:    88/88, Avg_loss: 1.483703, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622714, metric: 0.843483\n",
      "\n",
      "Epoch 74\n",
      "Batch:    88/88, Avg_loss: 1.482739, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.618088, metric: 0.848825\n",
      "\n",
      "Epoch 75\n",
      "Batch:    88/88, Avg_loss: 1.483432, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.618763, metric: 0.846421\n",
      "\n",
      "Epoch 76\n",
      "Batch:    88/88, Avg_loss: 1.482214, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615122, metric: 0.851763\n",
      "\n",
      "Epoch 77\n",
      "Batch:    88/88, Avg_loss: 1.482573, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.616197, metric: 0.848558\n",
      "\n",
      "Epoch 78\n",
      "Batch:    88/88, Avg_loss: 1.483472, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.620271, metric: 0.845085\n",
      "\n",
      "Epoch 79\n",
      "Batch:    88/88, Avg_loss: 1.483812, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.621667, metric: 0.844818\n",
      "\n",
      "Epoch 80\n",
      "Batch:    88/88, Avg_loss: 1.484077, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.619431, metric: 0.846421\n",
      "\n",
      "Epoch 81\n",
      "Batch:    88/88, Avg_loss: 1.482694, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.619299, metric: 0.844284\n",
      "\n",
      "Epoch 82\n",
      "Batch:    88/88, Avg_loss: 1.482371, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618041, metric: 0.847489\n",
      "\n",
      "Epoch 83\n",
      "Batch:    88/88, Avg_loss: 1.481716, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.615626, metric: 0.848558\n",
      "\n",
      "Epoch 84\n",
      "Batch:    88/88, Avg_loss: 1.481438, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.616841, metric: 0.846688\n",
      "\n",
      "Epoch 85\n",
      "Batch:    88/88, Avg_loss: 1.481537, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.611377, metric: 0.852564\n",
      "\n",
      "Epoch 86\n",
      "Batch:    88/88, Avg_loss: 1.481075, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.610768, metric: 0.854167\n",
      "\n",
      "Epoch 87\n",
      "Batch:    88/88, Avg_loss: 1.480871, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.610752, metric: 0.853632\n",
      "\n",
      "Epoch 88\n",
      "Batch:    88/88, Avg_loss: 1.480909, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.609407, metric: 0.854968\n",
      "\n",
      "Epoch 89\n",
      "Batch:    88/88, Avg_loss: 1.480939, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.614387, metric: 0.849359\n",
      "\n",
      "Epoch 90\n",
      "Batch:    88/88, Avg_loss: 1.481926, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.623044, metric: 0.842147\n",
      "\n",
      "Epoch 91\n",
      "Batch:    88/88, Avg_loss: 1.485680, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.618785, metric: 0.846421\n",
      "\n",
      "Epoch 92\n",
      "Batch:    88/88, Avg_loss: 1.485611, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.617995, metric: 0.845620\n",
      "\n",
      "Epoch 93\n",
      "Batch:    88/88, Avg_loss: 1.483649, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.618334, metric: 0.847489\n",
      "\n",
      "Epoch 94\n",
      "Batch:    88/88, Avg_loss: 1.482157, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.617525, metric: 0.848291\n",
      "\n",
      "Epoch 95\n",
      "Batch:    88/88, Avg_loss: 1.481319, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.616214, metric: 0.846688\n",
      "\n",
      "Epoch 96\n",
      "Batch:    88/88, Avg_loss: 1.482018, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.614650, metric: 0.846154\n",
      "\n",
      "Epoch 97\n",
      "Batch:    88/88, Avg_loss: 1.481711, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.612915, metric: 0.849359\n",
      "\n",
      "Epoch 98\n",
      "Batch:    88/88, Avg_loss: 1.481395, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.613265, metric: 0.851496\n",
      "\n",
      "Epoch 99\n",
      "Batch:    88/88, Avg_loss: 1.480978, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.611445, metric: 0.852030\n",
      "\n",
      "Epoch 100\n",
      "Batch:    88/88, Avg_loss: 1.481220, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.611289, metric: 0.848825\n",
      "\n",
      "Testing:\n",
      "loss: 1.596563, metric: 0.865990\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "model11 = Network_exprelu(layer_sizes).to(device)\n",
    "optimizer = optim.Adam(model11.parameters(), lr = 0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model11, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model11, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model11, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Scaled Exponential Linear Unit (SELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network_scaledrelu(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(Network_scaledrelu, self).__init__()\n",
    "        self.L1 = nn.Linear(in_features=layer_sizes[0], out_features=layer_sizes[1])\n",
    "        self.bn1 = nn.BatchNorm1d(layer_sizes[1])\n",
    "        self.L2 = nn.Linear(in_features=layer_sizes[1], out_features=layer_sizes[2])\n",
    "        self.bn2 = nn.BatchNorm1d(layer_sizes[2])\n",
    "        self.L3 = nn.Linear(in_features=layer_sizes[2], out_features=layer_sizes[3])\n",
    "        self.bn3 = nn.BatchNorm1d(layer_sizes[3])\n",
    "        self.L4 = nn.Linear(in_features=layer_sizes[3], out_features=layer_sizes[4])\n",
    "        self.bn4 = nn.BatchNorm1d(layer_sizes[4])\n",
    "        self.out = nn.Linear(in_features=layer_sizes[4], out_features=layer_sizes[5])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.L1.weight.dtype)\n",
    "        \n",
    "        L1 = F.selu(self.bn1(self.L1(x)))\n",
    "        L2 = F.selu(self.bn2(self.L2(L1)))\n",
    "        L3 = F.selu(self.bn3(self.L3(L2)))\n",
    "        L4 = F.selu(self.bn4(self.L4(L3)))\n",
    "        \n",
    "        out = F.softmax(self.out(L4), dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.827345, batch accuracy: 0.765306\n",
      "Validation:\n",
      "loss: 1.725183, metric: 0.818109\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.686120, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.686366, metric: 0.836806\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.648674, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.668546, metric: 0.842682\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.624816, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.658751, metric: 0.844551\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.606234, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.652660, metric: 0.847222\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.589059, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.644465, metric: 0.849626\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.576350, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.642992, metric: 0.843216\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.565608, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.638643, metric: 0.845887\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.557402, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.636761, metric: 0.848825\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.548746, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.637487, metric: 0.844284\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.542070, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.634413, metric: 0.845085\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.535168, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.632183, metric: 0.850427\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.530584, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.631159, metric: 0.847756\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.526944, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.631144, metric: 0.845620\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.522662, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.628316, metric: 0.848291\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.521389, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.631695, metric: 0.843750\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.517239, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.628730, metric: 0.846688\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.515040, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.628572, metric: 0.845353\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.513524, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.627504, metric: 0.844017\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.510443, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.627011, metric: 0.845085\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.509466, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.626408, metric: 0.846154\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.509738, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.628423, metric: 0.840278\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.508194, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.627334, metric: 0.846955\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.506258, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.627692, metric: 0.843750\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.505466, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.629642, metric: 0.839209\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.504877, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.626257, metric: 0.845620\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.503615, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.627977, metric: 0.842415\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.502713, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.628983, metric: 0.838675\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.501033, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.622822, metric: 0.847222\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.500172, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.624506, metric: 0.843750\n",
      "\n",
      "Epoch 31\n",
      "Batch:    88/88, Avg_loss: 1.499919, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.622053, metric: 0.848291\n",
      "\n",
      "Epoch 32\n",
      "Batch:    88/88, Avg_loss: 1.498487, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.621269, metric: 0.848291\n",
      "\n",
      "Epoch 33\n",
      "Batch:    88/88, Avg_loss: 1.497837, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.622558, metric: 0.844284\n",
      "\n",
      "Epoch 34\n",
      "Batch:    88/88, Avg_loss: 1.498373, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.624723, metric: 0.841880\n",
      "\n",
      "Epoch 35\n",
      "Batch:    88/88, Avg_loss: 1.498554, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.628939, metric: 0.838408\n",
      "\n",
      "Epoch 36\n",
      "Batch:    88/88, Avg_loss: 1.498937, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.626670, metric: 0.841346\n",
      "\n",
      "Epoch 37\n",
      "Batch:    88/88, Avg_loss: 1.499054, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.628483, metric: 0.839744\n",
      "\n",
      "Epoch 38\n",
      "Batch:    88/88, Avg_loss: 1.501386, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.630956, metric: 0.836004\n",
      "\n",
      "Epoch 39\n",
      "Batch:    88/88, Avg_loss: 1.499956, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.625883, metric: 0.841346\n",
      "\n",
      "Epoch 40\n",
      "Batch:    88/88, Avg_loss: 1.498064, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.624669, metric: 0.843750\n",
      "\n",
      "Epoch 41\n",
      "Batch:    88/88, Avg_loss: 1.495770, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.623176, metric: 0.846955\n",
      "\n",
      "Epoch 42\n",
      "Batch:    88/88, Avg_loss: 1.495378, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.621673, metric: 0.844017\n",
      "\n",
      "Epoch 43\n",
      "Batch:    88/88, Avg_loss: 1.494722, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.624342, metric: 0.842949\n",
      "\n",
      "Epoch 44\n",
      "Batch:    88/88, Avg_loss: 1.493589, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618911, metric: 0.848291\n",
      "\n",
      "Epoch 45\n",
      "Batch:    88/88, Avg_loss: 1.492918, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.619218, metric: 0.848558\n",
      "\n",
      "Epoch 46\n",
      "Batch:    88/88, Avg_loss: 1.493360, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.619331, metric: 0.847222\n",
      "\n",
      "Epoch 47\n",
      "Batch:    88/88, Avg_loss: 1.493589, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.628209, metric: 0.837340\n",
      "\n",
      "Epoch 48\n",
      "Batch:    88/88, Avg_loss: 1.494571, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.621942, metric: 0.843750\n",
      "\n",
      "Epoch 49\n",
      "Batch:    88/88, Avg_loss: 1.493687, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.622827, metric: 0.845085\n",
      "\n",
      "Epoch 50\n",
      "Batch:    88/88, Avg_loss: 1.492538, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.620604, metric: 0.844017\n",
      "\n",
      "Testing:\n",
      "loss: 1.603986, metric: 0.863855\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "model12 = Network_scaledrelu(layer_sizes).to(device)\n",
    "optimizer = optim.Adam(model12.parameters(), lr = 0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model12, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model12, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test (test_loader, model12, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all, the LeakyRelu gave us the mildly best performance but almost similar results as of all variants of ReLU! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Find the best L2 parameter (Weight Decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, metric, metric_name = 'accuracy'):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, metric_T = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            Yhat = model(X)\n",
    "            test_loss += loss_fn(Yhat, Y).item()\n",
    "            metric_T += metric(Yhat, Y)* len(Y)\n",
    "    test_loss /= num_batches\n",
    "    metric_T /= size\n",
    "    print(f\"loss: {test_loss:>7f}, metric: { metric_T:>5f}\")\n",
    "    return metric_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For weight_decay: 0.0001\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.805735, batch accuracy: 0.734694\n",
      "Validation:\n",
      "loss: 1.667326, metric: 0.845353\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.614441, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.629451, metric: 0.856036\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.573640, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.623562, metric: 0.853632\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.553700, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.614538, metric: 0.857372\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.540345, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.617125, metric: 0.851229\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.531648, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.609618, metric: 0.857372\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.526692, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.613296, metric: 0.853632\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.524158, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.616939, metric: 0.846688\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.522236, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.613876, metric: 0.851763\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.516164, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.614287, metric: 0.851229\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.514256, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.615734, metric: 0.847756\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.511434, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.613614, metric: 0.849893\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.512418, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.614781, metric: 0.848558\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.513869, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.606736, metric: 0.854701\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.509751, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.612522, metric: 0.854701\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.508318, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.608706, metric: 0.856571\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.507327, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.609031, metric: 0.854701\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.507337, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.610398, metric: 0.853900\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.506504, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.610021, metric: 0.854167\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.506232, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.604584, metric: 0.858440\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.504150, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.604050, metric: 0.858707\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.502598, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.610181, metric: 0.854434\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.505101, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.613230, metric: 0.849626\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.504370, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.607869, metric: 0.856303\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.505683, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.614409, metric: 0.845887\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.504884, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.609476, metric: 0.853632\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.505885, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.608034, metric: 0.854968\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.506263, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.606623, metric: 0.855769\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.501630, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.605886, metric: 0.857639\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.501559, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.604882, metric: 0.860577\n",
      "\n",
      "Testing:\n",
      "loss: 1.587885, metric: 0.873999\n",
      "Done!\n",
      "For weight_decay: 0.001\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.813320, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.664651, metric: 0.847489\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.619417, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.630623, metric: 0.852564\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.585902, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.631307, metric: 0.848024\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.569306, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.621842, metric: 0.851763\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.562940, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.622101, metric: 0.852030\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.556476, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.623704, metric: 0.848558\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.547771, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.620794, metric: 0.849893\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.546435, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.616881, metric: 0.851763\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.540710, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.622473, metric: 0.848291\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.540973, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.621122, metric: 0.849626\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.539586, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.627760, metric: 0.840278\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.538544, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.617853, metric: 0.851229\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.538181, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.618713, metric: 0.852297\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.538094, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.618974, metric: 0.850427\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.530812, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.619397, metric: 0.850160\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.528709, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.617719, metric: 0.852030\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.531546, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.612453, metric: 0.857372\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.529704, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.622548, metric: 0.846421\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.531295, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.616245, metric: 0.853365\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.526057, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.613239, metric: 0.851229\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.526045, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.612133, metric: 0.855502\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.527720, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.615033, metric: 0.853098\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.523628, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.615140, metric: 0.852564\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.523470, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.618663, metric: 0.847756\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.523599, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.618377, metric: 0.848558\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.519522, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.609815, metric: 0.856838\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.523302, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.614641, metric: 0.852831\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.522311, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.621100, metric: 0.842147\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.524351, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.611635, metric: 0.853900\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.518765, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.616668, metric: 0.854167\n",
      "\n",
      "Testing:\n",
      "loss: 1.601752, metric: 0.865190\n",
      "Done!\n",
      "For weight_decay: 0.01\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.826210, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.695638, metric: 0.832799\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.664075, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.669867, metric: 0.831998\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.639587, batch accuracy: 0.785714\n",
      "Validation:\n",
      "loss: 1.666146, metric: 0.825855\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.633746, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.667312, metric: 0.824786\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.633519, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 1.657319, metric: 0.835737\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.626839, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.663313, metric: 0.827991\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.626246, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.655400, metric: 0.835737\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.624662, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.658555, metric: 0.833066\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.624850, batch accuracy: 0.806122\n",
      "Validation:\n",
      "loss: 1.656337, metric: 0.837073\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.626603, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 1.653948, metric: 0.846421\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.622237, batch accuracy: 0.795918\n",
      "Validation:\n",
      "loss: 1.671507, metric: 0.829861\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.621153, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.653901, metric: 0.842147\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.619934, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.665520, metric: 0.831998\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.621587, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.662533, metric: 0.839744\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.615627, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.669412, metric: 0.834135\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.622774, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.663498, metric: 0.842147\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.617534, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.663064, metric: 0.843216\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.617933, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.661281, metric: 0.845353\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.617064, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.658036, metric: 0.850160\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.616120, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.673093, metric: 0.838942\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.617803, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.674807, metric: 0.835737\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.612747, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.664367, metric: 0.844551\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.615712, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 1.655828, metric: 0.847489\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.613548, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.655102, metric: 0.849626\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.612178, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.663874, metric: 0.844818\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.608997, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.657379, metric: 0.845353\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.610404, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.666196, metric: 0.841079\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.608378, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.660579, metric: 0.846154\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.605067, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.660276, metric: 0.847489\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.604155, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.657226, metric: 0.845887\n",
      "\n",
      "Testing:\n",
      "loss: 1.639941, metric: 0.860651\n",
      "Done!\n",
      "For weight_decay: 0.1\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.957935, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.801282, metric: 0.800748\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.783060, batch accuracy: 0.755102\n",
      "Validation:\n",
      "loss: 1.776689, metric: 0.797543\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.775808, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 1.791123, metric: 0.781250\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.773449, batch accuracy: 0.806122\n",
      "Validation:\n",
      "loss: 1.782310, metric: 0.785791\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.776460, batch accuracy: 0.785714\n",
      "Validation:\n",
      "loss: 1.790749, metric: 0.791132\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.780656, batch accuracy: 0.755102\n",
      "Validation:\n",
      "loss: 1.786640, metric: 0.797276\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.787942, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.795949, metric: 0.800214\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.788582, batch accuracy: 0.785714\n",
      "Validation:\n",
      "loss: 1.797336, metric: 0.798611\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.801551, batch accuracy: 0.775510\n",
      "Validation:\n",
      "loss: 1.810797, metric: 0.801282\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.809164, batch accuracy: 0.795918\n",
      "Validation:\n",
      "loss: 1.820502, metric: 0.790331\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.817266, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.825553, metric: 0.813568\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.829780, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 1.844171, metric: 0.807425\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.846388, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 1.855279, metric: 0.801282\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.859213, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.868826, metric: 0.818109\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.875876, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.895894, metric: 0.793002\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.894739, batch accuracy: 0.765306\n",
      "Validation:\n",
      "loss: 1.914969, metric: 0.799145\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.915171, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.928202, metric: 0.806624\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.936351, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.957447, metric: 0.795940\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.961601, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.977440, metric: 0.797543\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.987106, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 2.007485, metric: 0.796474\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 2.015450, batch accuracy: 0.795918\n",
      "Validation:\n",
      "loss: 2.034162, metric: 0.787660\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 2.047659, batch accuracy: 0.795918\n",
      "Validation:\n",
      "loss: 2.065520, metric: 0.795673\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 2.079827, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 2.099646, metric: 0.787393\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 2.111890, batch accuracy: 0.785714\n",
      "Validation:\n",
      "loss: 2.132038, metric: 0.791667\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 2.148389, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 2.166044, metric: 0.794605\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 2.184758, batch accuracy: 0.785714\n",
      "Validation:\n",
      "loss: 2.201919, metric: 0.791934\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 2.216535, batch accuracy: 0.744898\n",
      "Validation:\n",
      "loss: 2.232427, metric: 0.775107\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 2.243502, batch accuracy: 0.765306\n",
      "Validation:\n",
      "loss: 2.255408, metric: 0.784989\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 2.263997, batch accuracy: 0.755102\n",
      "Validation:\n",
      "loss: 2.272319, metric: 0.776976\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 2.277993, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 2.283633, metric: 0.765759\n",
      "\n",
      "Testing:\n",
      "loss: 2.283261, metric: 0.785638\n",
      "Done!\n",
      "For weight_decay: 1\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 2.171398, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 2.061084, metric: 0.759615\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 2.100466, batch accuracy: 0.714286\n",
      "Validation:\n",
      "loss: 2.140612, metric: 0.689103\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 2.152678, batch accuracy: 0.561224\n",
      "Validation:\n",
      "loss: 2.166783, metric: 0.591880\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 2.170738, batch accuracy: 0.602041\n",
      "Validation:\n",
      "loss: 2.177136, metric: 0.551015\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 2.180969, batch accuracy: 0.551020\n",
      "Validation:\n",
      "loss: 2.188989, metric: 0.546207\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 2.195709, batch accuracy: 0.571429\n",
      "Validation:\n",
      "loss: 2.204336, metric: 0.532318\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 2.207892, batch accuracy: 0.438776\n",
      "Validation:\n",
      "loss: 2.218968, metric: 0.532853\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 2.223557, batch accuracy: 0.479592\n",
      "Validation:\n",
      "loss: 2.227231, metric: 0.532585\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 2.234210, batch accuracy: 0.520408\n",
      "Validation:\n",
      "loss: 2.243079, metric: 0.517895\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 2.247458, batch accuracy: 0.561224\n",
      "Validation:\n",
      "loss: 2.254543, metric: 0.517361\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 2.259507, batch accuracy: 0.520408\n",
      "Validation:\n",
      "loss: 2.266393, metric: 0.520566\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 2.269693, batch accuracy: 0.581633\n",
      "Validation:\n",
      "loss: 2.274284, metric: 0.527511\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 2.278029, batch accuracy: 0.540816\n",
      "Validation:\n",
      "loss: 2.282627, metric: 0.501870\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 2.284437, batch accuracy: 0.479592\n",
      "Validation:\n",
      "loss: 2.287216, metric: 0.528045\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 2.289036, batch accuracy: 0.622449\n",
      "Validation:\n",
      "loss: 2.290968, metric: 0.534455\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 2.292187, batch accuracy: 0.520408\n",
      "Validation:\n",
      "loss: 2.293567, metric: 0.516827\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 2.294458, batch accuracy: 0.520408\n",
      "Validation:\n",
      "loss: 2.295746, metric: 0.535524\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 2.296483, batch accuracy: 0.408163\n",
      "Validation:\n",
      "loss: 2.297413, metric: 0.467415\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 2.298027, batch accuracy: 0.438776\n",
      "Validation:\n",
      "loss: 2.298520, metric: 0.439904\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 2.298972, batch accuracy: 0.479592\n",
      "Validation:\n",
      "loss: 2.299758, metric: 0.395566\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 2.300076, batch accuracy: 0.377551\n",
      "Validation:\n",
      "loss: 2.300766, metric: 0.323718\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 2.301082, batch accuracy: 0.397959\n",
      "Validation:\n",
      "loss: 2.301373, metric: 0.313835\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 2.301578, batch accuracy: 0.244898\n",
      "Validation:\n",
      "loss: 2.301850, metric: 0.256677\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 2.301636, batch accuracy: 0.142857\n",
      "Validation:\n",
      "loss: 2.301178, metric: 0.169605\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 2.300806, batch accuracy: 0.112245\n",
      "Validation:\n",
      "loss: 2.300826, metric: 0.096154\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 2.301092, batch accuracy: 0.091837\n",
      "Validation:\n",
      "loss: 2.301531, metric: 0.096154\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 2.302029, batch accuracy: 0.142857\n",
      "Validation:\n",
      "loss: 2.302519, metric: 0.170139\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 2.302582, batch accuracy: 0.102041\n",
      "Validation:\n",
      "loss: 2.302585, metric: 0.097222\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 2.302586, batch accuracy: 0.071429\n",
      "Validation:\n",
      "loss: 2.302585, metric: 0.105235\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 2.302586, batch accuracy: 0.142857\n",
      "Validation:\n",
      "loss: 2.302587, metric: 0.091880\n",
      "\n",
      "Testing:\n",
      "loss: 2.302586, metric: 0.099306\n",
      "Done!\n",
      "Best Weight Decay value: 0.0001\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [28*28, 1000, 500, 250, 100, 10]\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "weight_decay_values = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "best_weight_decay = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for weight_decay in weight_decay_values:\n",
    "    model13 = Network_leakyrelu(layer_sizes).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model13.parameters(), lr=0.0005, weight_decay=weight_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    print(f\"For weight_decay: {weight_decay}\")\n",
    "    epochs = 30\n",
    "    for t in range(epochs):\n",
    "        print(f\"\\nEpoch {t+1}\")\n",
    "        train(train_loader, model13, loss_fn, optimizer, accuracy)\n",
    "        print('Validation:')\n",
    "        test(valid_loader, model13, loss_fn, accuracy)\n",
    "    print('\\nTesting:')\n",
    "    test_accuracy = test(test_loader, model13, loss_fn, accuracy)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_weight_decay = weight_decay\n",
    "        best_accuracy = test_accuracy\n",
    "        \n",
    "print(f\"Best Weight Decay value: {best_weight_decay}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this experiment, I found that the L2 parameter i.e. (weight decay) should be within 10^-4 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For weight_decay: 0.0001\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.822139, batch accuracy: 0.755102\n",
      "Validation:\n",
      "loss: 1.665808, metric: 0.846421\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.615333, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.627127, metric: 0.857372\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.574083, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.620839, metric: 0.852831\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.552578, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.613854, metric: 0.854701\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.540468, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.618232, metric: 0.850962\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.531559, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.616564, metric: 0.852564\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.527924, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.609350, metric: 0.858173\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.523083, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.610055, metric: 0.855502\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.522067, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.616732, metric: 0.847222\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.519670, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.607008, metric: 0.856838\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.515309, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.610205, metric: 0.853632\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.513953, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.614839, metric: 0.852030\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.513539, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.611788, metric: 0.852831\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.510591, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.610621, metric: 0.855769\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.511359, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.606718, metric: 0.857906\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.509792, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.608734, metric: 0.857105\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.507804, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.604775, metric: 0.861378\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.503804, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.611620, metric: 0.851229\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.504408, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.613740, metric: 0.848558\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.503005, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.610756, metric: 0.852564\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.503152, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.608049, metric: 0.853098\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.504646, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.606613, metric: 0.856303\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.503408, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.609098, metric: 0.854701\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.503939, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.618862, metric: 0.844017\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.502738, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.605783, metric: 0.857639\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.502171, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.609643, metric: 0.853365\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.504420, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.610639, metric: 0.854968\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.503290, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.605804, metric: 0.859241\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.501556, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.603050, metric: 0.858440\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.500690, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.610021, metric: 0.851496\n",
      "\n",
      "Testing:\n",
      "loss: 1.593415, metric: 0.867325\n",
      "Done!\n",
      "For weight_decay: 0.0002\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.815718, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.662015, metric: 0.849893\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.618145, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 1.626135, metric: 0.853098\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.573109, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.621259, metric: 0.853900\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.552574, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.617656, metric: 0.852831\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.544442, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.617816, metric: 0.849626\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.534227, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.616136, metric: 0.850160\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.529709, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.616238, metric: 0.849359\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.530619, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.615674, metric: 0.850160\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.524038, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.610338, metric: 0.854434\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.521552, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.609839, metric: 0.855502\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.519587, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.608449, metric: 0.858173\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.515329, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.611350, metric: 0.854167\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.517753, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.609815, metric: 0.856036\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.514714, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.608772, metric: 0.855769\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.514609, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.607293, metric: 0.860043\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.512986, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.605875, metric: 0.856303\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.513057, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.606300, metric: 0.856571\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.512117, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.611629, metric: 0.850694\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.511236, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.616971, metric: 0.847756\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.510530, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.609807, metric: 0.854434\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.508424, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.605588, metric: 0.858974\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.508233, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.609559, metric: 0.857372\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.508414, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.615959, metric: 0.849893\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.508689, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.606450, metric: 0.856838\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.507106, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.610632, metric: 0.853632\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.507451, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.611759, metric: 0.851496\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.504279, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.608247, metric: 0.856838\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.505299, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.608190, metric: 0.854434\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.507001, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.612388, metric: 0.852564\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.506005, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.610951, metric: 0.852564\n",
      "\n",
      "Testing:\n",
      "loss: 1.596892, metric: 0.861986\n",
      "Done!\n",
      "For weight_decay: 0.0003\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.825427, batch accuracy: 0.816326\n",
      "Validation:\n",
      "loss: 1.665834, metric: 0.850962\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.619354, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.632696, metric: 0.851496\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.573152, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.624352, metric: 0.853900\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.556635, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.622321, metric: 0.852831\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.544162, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.616591, metric: 0.855502\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.535612, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.616537, metric: 0.850160\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.532972, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.615558, metric: 0.851229\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.528549, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.619201, metric: 0.848291\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.526818, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.616763, metric: 0.847222\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.525716, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.612793, metric: 0.854701\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.522552, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.612285, metric: 0.854701\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.521594, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.616041, metric: 0.849893\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.518100, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.613767, metric: 0.850694\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.518319, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.608753, metric: 0.857639\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.514322, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.604715, metric: 0.860310\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.511954, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.607316, metric: 0.857372\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.516559, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.607967, metric: 0.860043\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.515168, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.611514, metric: 0.853098\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.512147, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.605755, metric: 0.860310\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.510161, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.617855, metric: 0.845887\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.509619, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.615129, metric: 0.848024\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.509140, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.608106, metric: 0.856571\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.508367, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.613158, metric: 0.851496\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.511949, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.610761, metric: 0.853900\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.511509, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.614773, metric: 0.848825\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.512429, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.609225, metric: 0.855235\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.506254, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.615223, metric: 0.849359\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.505856, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.610541, metric: 0.852297\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.506455, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.612919, metric: 0.850962\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.504258, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.610097, metric: 0.854434\n",
      "\n",
      "Testing:\n",
      "loss: 1.597529, metric: 0.863855\n",
      "Done!\n",
      "For weight_decay: 0.0004\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.808198, batch accuracy: 0.765306\n",
      "Validation:\n",
      "loss: 1.667090, metric: 0.846688\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.618917, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.631053, metric: 0.856036\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.574813, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.626652, metric: 0.850427\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.555799, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.617923, metric: 0.854701\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.547292, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.621211, metric: 0.847222\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.539786, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.628335, metric: 0.839209\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.535757, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.612558, metric: 0.857372\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.532222, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.617208, metric: 0.853098\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.530051, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.614002, metric: 0.853098\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.528242, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.616907, metric: 0.850962\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.525250, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.614053, metric: 0.853098\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.522780, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.616808, metric: 0.846421\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.525604, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.615347, metric: 0.850427\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.520768, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.611949, metric: 0.851229\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.522679, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.617165, metric: 0.848558\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.519611, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.610630, metric: 0.851496\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.518675, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.610786, metric: 0.854434\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.514641, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.616036, metric: 0.846421\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.516817, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.619266, metric: 0.846955\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.516131, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.614389, metric: 0.849626\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.515462, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.614284, metric: 0.850962\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.512897, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.618507, metric: 0.846154\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.515432, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.615182, metric: 0.848291\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.511842, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.620878, metric: 0.842682\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.511431, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.613056, metric: 0.847756\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.514190, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.613002, metric: 0.851496\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.515929, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.607761, metric: 0.855235\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.511417, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.614427, metric: 0.849092\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.509110, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.618203, metric: 0.845620\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.509430, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.617641, metric: 0.847222\n",
      "\n",
      "Testing:\n",
      "loss: 1.597941, metric: 0.868126\n",
      "Done!\n",
      "For weight_decay: 0.0005\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.809733, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.662035, metric: 0.847756\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.618013, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.634511, metric: 0.846154\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.575083, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.620554, metric: 0.854968\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.558138, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.619940, metric: 0.851763\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.549411, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.616971, metric: 0.852564\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.544945, batch accuracy: 0.826531\n",
      "Validation:\n",
      "loss: 1.614318, metric: 0.853098\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.535727, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.610966, metric: 0.859776\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.534248, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.618069, metric: 0.848558\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.532435, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.617223, metric: 0.849359\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.528872, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.609408, metric: 0.857105\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.526886, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.614491, metric: 0.851763\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.528699, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.610305, metric: 0.855769\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.522880, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.606396, metric: 0.858707\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.522103, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.619280, metric: 0.846688\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.519362, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.614897, metric: 0.849893\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.519360, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.618311, metric: 0.847489\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.521357, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.624317, metric: 0.841079\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.518817, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.614881, metric: 0.850962\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.518009, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.609414, metric: 0.855502\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.516519, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.612792, metric: 0.853900\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.515839, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.618337, metric: 0.848024\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.517409, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.608345, metric: 0.857105\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.519137, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.607295, metric: 0.856303\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.514311, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.610852, metric: 0.856303\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.511819, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.617916, metric: 0.847756\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.513111, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.614486, metric: 0.850160\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.512468, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.609953, metric: 0.856036\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.514628, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.614821, metric: 0.856571\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.513420, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.612849, metric: 0.850160\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.509806, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.610105, metric: 0.855235\n",
      "\n",
      "Testing:\n",
      "loss: 1.598079, metric: 0.864923\n",
      "Done!\n",
      "For weight_decay: 0.0006\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.818727, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.668126, metric: 0.845887\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.619330, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.630323, metric: 0.852030\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.577981, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.620785, metric: 0.855502\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.558937, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.618405, metric: 0.853365\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.550789, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.630706, metric: 0.839744\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.547233, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.622270, metric: 0.845887\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.540004, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.623474, metric: 0.842949\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.537828, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.613967, metric: 0.853632\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.532800, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.614740, metric: 0.854968\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.530810, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.620302, metric: 0.846955\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.528307, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.617179, metric: 0.849626\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.525562, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.619364, metric: 0.848291\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.527559, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.616450, metric: 0.851229\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.527843, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.612484, metric: 0.854434\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.525727, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.618124, metric: 0.850427\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.523312, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.611832, metric: 0.856838\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.524428, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.614850, metric: 0.851763\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.517153, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.614367, metric: 0.853365\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.519085, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.610924, metric: 0.854701\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.515472, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.615946, metric: 0.850160\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.518623, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.617122, metric: 0.847756\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.516961, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.616967, metric: 0.850160\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.518519, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.614262, metric: 0.850962\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.515901, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.609545, metric: 0.859776\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.514116, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.617975, metric: 0.848558\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.517594, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.624440, metric: 0.842949\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.512884, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.613797, metric: 0.854167\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.511275, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.613294, metric: 0.852030\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.511743, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.611068, metric: 0.855769\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.515564, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.617280, metric: 0.851496\n",
      "\n",
      "Testing:\n",
      "loss: 1.598711, metric: 0.865990\n",
      "Done!\n",
      "For weight_decay: 0.0007\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.809524, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.665597, metric: 0.849626\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.620057, batch accuracy: 0.857143\n",
      "Validation:\n",
      "loss: 1.632702, metric: 0.852564\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.577760, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.625168, metric: 0.848024\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.561373, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.626181, metric: 0.847756\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.553359, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.622358, metric: 0.847489\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.551372, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.620454, metric: 0.850160\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.545336, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.618528, metric: 0.850962\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.540732, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.617778, metric: 0.847756\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.536823, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.634725, metric: 0.834936\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.535567, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.618672, metric: 0.849893\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.527990, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.613568, metric: 0.854167\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.530166, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.619327, metric: 0.845353\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.531546, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.614644, metric: 0.852030\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.530404, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.627228, metric: 0.841613\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.527533, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.620269, metric: 0.846688\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.524568, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.628781, metric: 0.840812\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.523139, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.615357, metric: 0.849092\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.521943, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.619668, metric: 0.846688\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.520819, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.620855, metric: 0.846955\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.520686, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.618249, metric: 0.849893\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.522286, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.617781, metric: 0.847756\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.518187, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.614830, metric: 0.850160\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.521507, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.619412, metric: 0.848825\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.519681, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.617103, metric: 0.852564\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.513714, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.618172, metric: 0.848024\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.514571, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.616478, metric: 0.851496\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.517045, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.612617, metric: 0.854434\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.515791, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.619171, metric: 0.843216\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.515886, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.616215, metric: 0.849893\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.516392, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.614322, metric: 0.849893\n",
      "\n",
      "Testing:\n",
      "loss: 1.595971, metric: 0.869461\n",
      "Done!\n",
      "For weight_decay: 0.0008\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.812722, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.673316, metric: 0.844551\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.620993, batch accuracy: 0.806122\n",
      "Validation:\n",
      "loss: 1.633590, metric: 0.849359\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.580294, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.622003, metric: 0.854434\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.563062, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.626917, metric: 0.846688\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.556929, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.622670, metric: 0.849359\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.550539, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.624970, metric: 0.843216\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.543267, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.618383, metric: 0.852030\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.542933, batch accuracy: 0.836735\n",
      "Validation:\n",
      "loss: 1.615268, metric: 0.852564\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.539245, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.628116, metric: 0.840812\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.536577, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.626000, metric: 0.843216\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.536538, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.617142, metric: 0.848825\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.529009, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.621859, metric: 0.842682\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.532593, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.614525, metric: 0.851763\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.530504, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.620975, metric: 0.847489\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.528685, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.615201, metric: 0.855235\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.525294, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.613274, metric: 0.852564\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.527601, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.620886, metric: 0.849359\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.525501, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.614506, metric: 0.853632\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.524691, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.624167, metric: 0.844017\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.526992, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.613885, metric: 0.853098\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.524693, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.609150, metric: 0.859776\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.525422, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.617213, metric: 0.849359\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.521692, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.615425, metric: 0.850962\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.520012, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.616502, metric: 0.851763\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.522213, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.617672, metric: 0.852030\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.517096, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.614017, metric: 0.852831\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.522071, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.628470, metric: 0.839744\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.520827, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.615048, metric: 0.851496\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.514768, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.610995, metric: 0.854434\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.513630, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.605057, metric: 0.862714\n",
      "\n",
      "Testing:\n",
      "loss: 1.598032, metric: 0.865723\n",
      "Done!\n",
      "For weight_decay: 0.0009\n",
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.815094, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.670561, metric: 0.843483\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.620365, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.636175, metric: 0.849893\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.579829, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.621473, metric: 0.854434\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.564653, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.625629, metric: 0.850160\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.556978, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.623521, metric: 0.847489\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.552766, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.616699, metric: 0.853098\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.546330, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.622059, metric: 0.846421\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.543788, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.618530, metric: 0.852831\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.538526, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.619485, metric: 0.850962\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.540254, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.619372, metric: 0.850427\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.536636, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.618251, metric: 0.849092\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.534231, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.612964, metric: 0.855502\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.528488, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.620657, metric: 0.850962\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.531611, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.624706, metric: 0.844551\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.532662, batch accuracy: 0.867347\n",
      "Validation:\n",
      "loss: 1.613169, metric: 0.856571\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.526222, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.617012, metric: 0.851229\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.527421, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.623313, metric: 0.844551\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.524959, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.615331, metric: 0.852831\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.525879, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.621903, metric: 0.848291\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.524000, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.612900, metric: 0.853632\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.523736, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.613725, metric: 0.853900\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.525303, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.614948, metric: 0.853900\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.520198, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.610189, metric: 0.857372\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.522729, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.622422, metric: 0.845085\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.522241, batch accuracy: 0.877551\n",
      "Validation:\n",
      "loss: 1.619684, metric: 0.845620\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.516965, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.611335, metric: 0.853365\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.521554, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.618666, metric: 0.847756\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.518340, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.607766, metric: 0.860577\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.517693, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.617184, metric: 0.849359\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.516885, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.617946, metric: 0.849893\n",
      "\n",
      "Testing:\n",
      "loss: 1.603005, metric: 0.864923\n",
      "Done!\n",
      "Best weight_decay value:0.0007\n"
     ]
    }
   ],
   "source": [
    "# Here, let's experiment in a broader range:\n",
    "weight_decay_values = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009]\n",
    "best_weight_decay = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for weight_decay in weight_decay_values:\n",
    "    model14 = Network_leakyrelu(layer_sizes).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model14.parameters(), lr = 0.0005, weight_decay = weight_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    print(f\"For weight_decay: {weight_decay}\")\n",
    "    epochs = 30\n",
    "    for t in range(epochs):\n",
    "        print(f\"\\nEpoch {t+1}\")\n",
    "        train(train_loader, model14, loss_fn, optimizer, accuracy)\n",
    "        print('Validation:')\n",
    "        test (valid_loader, model14, loss_fn, accuracy)\n",
    "    print('\\nTesting:')\n",
    "    test_accuracy = test (test_loader, model14, loss_fn, accuracy)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    \n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_weight_decay = weight_decay\n",
    "        best_accuracy = test_accuracy\n",
    "        \n",
    "print(f\"Best weight_decay value:{best_weight_decay}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our top-performing model. From previous experiments, the highest test accuracy (0.873) was achieved with model7. This model incorporated batch normalization in its layers and used a simple ReLU as the activation function for the hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch:    88/88, Avg_loss: 1.807764, batch accuracy: 0.734694\n",
      "Validation:\n",
      "loss: 1.667985, metric: 0.844818\n",
      "\n",
      "Epoch 2\n",
      "Batch:    88/88, Avg_loss: 1.614618, batch accuracy: 0.846939\n",
      "Validation:\n",
      "loss: 1.630073, metric: 0.852564\n",
      "\n",
      "Epoch 3\n",
      "Batch:    88/88, Avg_loss: 1.572167, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.623283, metric: 0.851763\n",
      "\n",
      "Epoch 4\n",
      "Batch:    88/88, Avg_loss: 1.551770, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.617768, metric: 0.852030\n",
      "\n",
      "Epoch 5\n",
      "Batch:    88/88, Avg_loss: 1.537925, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.617521, metric: 0.848558\n",
      "\n",
      "Epoch 6\n",
      "Batch:    88/88, Avg_loss: 1.528531, batch accuracy: 0.887755\n",
      "Validation:\n",
      "loss: 1.611706, metric: 0.857639\n",
      "\n",
      "Epoch 7\n",
      "Batch:    88/88, Avg_loss: 1.524568, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.613668, metric: 0.851763\n",
      "\n",
      "Epoch 8\n",
      "Batch:    88/88, Avg_loss: 1.521541, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.609747, metric: 0.856571\n",
      "\n",
      "Epoch 9\n",
      "Batch:    88/88, Avg_loss: 1.518321, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.607004, metric: 0.858974\n",
      "\n",
      "Epoch 10\n",
      "Batch:    88/88, Avg_loss: 1.514801, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.609229, metric: 0.855502\n",
      "\n",
      "Epoch 11\n",
      "Batch:    88/88, Avg_loss: 1.512148, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.610374, metric: 0.854701\n",
      "\n",
      "Epoch 12\n",
      "Batch:    88/88, Avg_loss: 1.509944, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.609263, metric: 0.854968\n",
      "\n",
      "Epoch 13\n",
      "Batch:    88/88, Avg_loss: 1.510455, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.607768, metric: 0.856571\n",
      "\n",
      "Epoch 14\n",
      "Batch:    88/88, Avg_loss: 1.507455, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.609496, metric: 0.853098\n",
      "\n",
      "Epoch 15\n",
      "Batch:    88/88, Avg_loss: 1.506081, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.607321, metric: 0.852564\n",
      "\n",
      "Epoch 16\n",
      "Batch:    88/88, Avg_loss: 1.504494, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.606900, metric: 0.855502\n",
      "\n",
      "Epoch 17\n",
      "Batch:    88/88, Avg_loss: 1.504122, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.605648, metric: 0.856571\n",
      "\n",
      "Epoch 18\n",
      "Batch:    88/88, Avg_loss: 1.502007, batch accuracy: 0.897959\n",
      "Validation:\n",
      "loss: 1.605414, metric: 0.857372\n",
      "\n",
      "Epoch 19\n",
      "Batch:    88/88, Avg_loss: 1.501074, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.609055, metric: 0.852297\n",
      "\n",
      "Epoch 20\n",
      "Batch:    88/88, Avg_loss: 1.501570, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.609673, metric: 0.853098\n",
      "\n",
      "Epoch 21\n",
      "Batch:    88/88, Avg_loss: 1.502864, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.609185, metric: 0.852831\n",
      "\n",
      "Epoch 22\n",
      "Batch:    88/88, Avg_loss: 1.502798, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.613498, metric: 0.850427\n",
      "\n",
      "Epoch 23\n",
      "Batch:    88/88, Avg_loss: 1.503111, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.607715, metric: 0.856571\n",
      "\n",
      "Epoch 24\n",
      "Batch:    88/88, Avg_loss: 1.501746, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.604460, metric: 0.861645\n",
      "\n",
      "Epoch 25\n",
      "Batch:    88/88, Avg_loss: 1.498826, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.599801, metric: 0.859509\n",
      "\n",
      "Epoch 26\n",
      "Batch:    88/88, Avg_loss: 1.500901, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.606348, metric: 0.858173\n",
      "\n",
      "Epoch 27\n",
      "Batch:    88/88, Avg_loss: 1.499717, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.604403, metric: 0.857906\n",
      "\n",
      "Epoch 28\n",
      "Batch:    88/88, Avg_loss: 1.497933, batch accuracy: 0.918367\n",
      "Validation:\n",
      "loss: 1.603250, metric: 0.857906\n",
      "\n",
      "Epoch 29\n",
      "Batch:    88/88, Avg_loss: 1.498796, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.601897, metric: 0.860844\n",
      "\n",
      "Epoch 30\n",
      "Batch:    88/88, Avg_loss: 1.497811, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.602847, metric: 0.860844\n",
      "\n",
      "Epoch 31\n",
      "Batch:    88/88, Avg_loss: 1.497667, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.601206, metric: 0.858707\n",
      "\n",
      "Epoch 32\n",
      "Batch:    88/88, Avg_loss: 1.497612, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.600047, metric: 0.863515\n",
      "\n",
      "Epoch 33\n",
      "Batch:    88/88, Avg_loss: 1.496598, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.595590, metric: 0.868323\n",
      "\n",
      "Epoch 34\n",
      "Batch:    88/88, Avg_loss: 1.495626, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.600176, metric: 0.863782\n",
      "\n",
      "Epoch 35\n",
      "Batch:    88/88, Avg_loss: 1.495403, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.599365, metric: 0.864850\n",
      "\n",
      "Epoch 36\n",
      "Batch:    88/88, Avg_loss: 1.494270, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.596620, metric: 0.866720\n",
      "\n",
      "Epoch 37\n",
      "Batch:    88/88, Avg_loss: 1.494926, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.600428, metric: 0.862447\n",
      "\n",
      "Epoch 38\n",
      "Batch:    88/88, Avg_loss: 1.494959, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.598818, metric: 0.864850\n",
      "\n",
      "Epoch 39\n",
      "Batch:    88/88, Avg_loss: 1.494866, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.595125, metric: 0.865385\n",
      "\n",
      "Epoch 40\n",
      "Batch:    88/88, Avg_loss: 1.495693, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.600208, metric: 0.860844\n",
      "\n",
      "Epoch 41\n",
      "Batch:    88/88, Avg_loss: 1.497654, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.601836, metric: 0.859776\n",
      "\n",
      "Epoch 42\n",
      "Batch:    88/88, Avg_loss: 1.496401, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.601388, metric: 0.860310\n",
      "\n",
      "Epoch 43\n",
      "Batch:    88/88, Avg_loss: 1.496118, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.599779, metric: 0.861645\n",
      "\n",
      "Epoch 44\n",
      "Batch:    88/88, Avg_loss: 1.493990, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.601377, metric: 0.861645\n",
      "\n",
      "Epoch 45\n",
      "Batch:    88/88, Avg_loss: 1.494406, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.593888, metric: 0.869124\n",
      "\n",
      "Epoch 46\n",
      "Batch:    88/88, Avg_loss: 1.493000, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.599272, metric: 0.864583\n",
      "\n",
      "Epoch 47\n",
      "Batch:    88/88, Avg_loss: 1.494532, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.599786, metric: 0.860844\n",
      "\n",
      "Epoch 48\n",
      "Batch:    88/88, Avg_loss: 1.493101, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.598320, metric: 0.862447\n",
      "\n",
      "Epoch 49\n",
      "Batch:    88/88, Avg_loss: 1.493215, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.599457, metric: 0.865385\n",
      "\n",
      "Epoch 50\n",
      "Batch:    88/88, Avg_loss: 1.493770, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.597039, metric: 0.865118\n",
      "\n",
      "Epoch 51\n",
      "Batch:    88/88, Avg_loss: 1.494625, batch accuracy: 0.928571\n",
      "Validation:\n",
      "loss: 1.602721, metric: 0.857372\n",
      "\n",
      "Epoch 52\n",
      "Batch:    88/88, Avg_loss: 1.492473, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.598521, metric: 0.862714\n",
      "\n",
      "Epoch 53\n",
      "Batch:    88/88, Avg_loss: 1.493567, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.600327, metric: 0.860844\n",
      "\n",
      "Epoch 54\n",
      "Batch:    88/88, Avg_loss: 1.492697, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.598364, metric: 0.863782\n",
      "\n",
      "Epoch 55\n",
      "Batch:    88/88, Avg_loss: 1.492886, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.601332, metric: 0.859509\n",
      "\n",
      "Epoch 56\n",
      "Batch:    88/88, Avg_loss: 1.492114, batch accuracy: 0.908163\n",
      "Validation:\n",
      "loss: 1.601929, metric: 0.860310\n",
      "\n",
      "Epoch 57\n",
      "Batch:    88/88, Avg_loss: 1.491998, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.601679, metric: 0.859241\n",
      "\n",
      "Epoch 58\n",
      "Batch:    88/88, Avg_loss: 1.491877, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.605758, metric: 0.857105\n",
      "\n",
      "Epoch 59\n",
      "Batch:    88/88, Avg_loss: 1.490272, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.604015, metric: 0.858173\n",
      "\n",
      "Epoch 60\n",
      "Batch:    88/88, Avg_loss: 1.490434, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.605393, metric: 0.857105\n",
      "\n",
      "Epoch 61\n",
      "Batch:    88/88, Avg_loss: 1.491467, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.601495, metric: 0.860310\n",
      "\n",
      "Epoch 62\n",
      "Batch:    88/88, Avg_loss: 1.491648, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.607162, metric: 0.855235\n",
      "\n",
      "Epoch 63\n",
      "Batch:    88/88, Avg_loss: 1.492318, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.604532, metric: 0.858440\n",
      "\n",
      "Epoch 64\n",
      "Batch:    88/88, Avg_loss: 1.491687, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.603547, metric: 0.856838\n",
      "\n",
      "Epoch 65\n",
      "Batch:    88/88, Avg_loss: 1.490880, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.603977, metric: 0.857639\n",
      "\n",
      "Epoch 66\n",
      "Batch:    88/88, Avg_loss: 1.491342, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.604201, metric: 0.856571\n",
      "\n",
      "Epoch 67\n",
      "Batch:    88/88, Avg_loss: 1.490696, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.601855, metric: 0.860577\n",
      "\n",
      "Epoch 68\n",
      "Batch:    88/88, Avg_loss: 1.489610, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.603786, metric: 0.857372\n",
      "\n",
      "Epoch 69\n",
      "Batch:    88/88, Avg_loss: 1.490430, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.604458, metric: 0.858440\n",
      "\n",
      "Epoch 70\n",
      "Batch:    88/88, Avg_loss: 1.490876, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.606803, metric: 0.854434\n",
      "\n",
      "Epoch 71\n",
      "Batch:    88/88, Avg_loss: 1.490642, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.607150, metric: 0.855235\n",
      "\n",
      "Epoch 72\n",
      "Batch:    88/88, Avg_loss: 1.490841, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.606412, metric: 0.852831\n",
      "\n",
      "Epoch 73\n",
      "Batch:    88/88, Avg_loss: 1.491613, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.604058, metric: 0.858173\n",
      "\n",
      "Epoch 74\n",
      "Batch:    88/88, Avg_loss: 1.491608, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.607556, metric: 0.855769\n",
      "\n",
      "Epoch 75\n",
      "Batch:    88/88, Avg_loss: 1.491976, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.602570, metric: 0.858974\n",
      "\n",
      "Epoch 76\n",
      "Batch:    88/88, Avg_loss: 1.491540, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.609600, metric: 0.850427\n",
      "\n",
      "Epoch 77\n",
      "Batch:    88/88, Avg_loss: 1.491201, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.602936, metric: 0.858974\n",
      "\n",
      "Epoch 78\n",
      "Batch:    88/88, Avg_loss: 1.490629, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.601036, metric: 0.860043\n",
      "\n",
      "Epoch 79\n",
      "Batch:    88/88, Avg_loss: 1.490043, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.602622, metric: 0.860043\n",
      "\n",
      "Epoch 80\n",
      "Batch:    88/88, Avg_loss: 1.490610, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.600917, metric: 0.861645\n",
      "\n",
      "Epoch 81\n",
      "Batch:    88/88, Avg_loss: 1.491002, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.601478, metric: 0.859776\n",
      "\n",
      "Epoch 82\n",
      "Batch:    88/88, Avg_loss: 1.489281, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.599470, metric: 0.864049\n",
      "\n",
      "Epoch 83\n",
      "Batch:    88/88, Avg_loss: 1.490935, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.601684, metric: 0.861111\n",
      "\n",
      "Epoch 84\n",
      "Batch:    88/88, Avg_loss: 1.490705, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.603680, metric: 0.857906\n",
      "\n",
      "Epoch 85\n",
      "Batch:    88/88, Avg_loss: 1.490327, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.600786, metric: 0.862179\n",
      "\n",
      "Epoch 86\n",
      "Batch:    88/88, Avg_loss: 1.490078, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.600036, metric: 0.861645\n",
      "\n",
      "Epoch 87\n",
      "Batch:    88/88, Avg_loss: 1.489357, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.599541, metric: 0.863248\n",
      "\n",
      "Epoch 88\n",
      "Batch:    88/88, Avg_loss: 1.489054, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.599210, metric: 0.865652\n",
      "\n",
      "Epoch 89\n",
      "Batch:    88/88, Avg_loss: 1.489890, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.603300, metric: 0.859241\n",
      "\n",
      "Epoch 90\n",
      "Batch:    88/88, Avg_loss: 1.491153, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.608523, metric: 0.852831\n",
      "\n",
      "Epoch 91\n",
      "Batch:    88/88, Avg_loss: 1.489775, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.599170, metric: 0.860310\n",
      "\n",
      "Epoch 92\n",
      "Batch:    88/88, Avg_loss: 1.488681, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.601040, metric: 0.860043\n",
      "\n",
      "Epoch 93\n",
      "Batch:    88/88, Avg_loss: 1.488922, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.603631, metric: 0.858440\n",
      "\n",
      "Epoch 94\n",
      "Batch:    88/88, Avg_loss: 1.488468, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.602611, metric: 0.859509\n",
      "\n",
      "Epoch 95\n",
      "Batch:    88/88, Avg_loss: 1.488629, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.602411, metric: 0.860043\n",
      "\n",
      "Epoch 96\n",
      "Batch:    88/88, Avg_loss: 1.489262, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.606014, metric: 0.856036\n",
      "\n",
      "Epoch 97\n",
      "Batch:    88/88, Avg_loss: 1.490256, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.598556, metric: 0.862447\n",
      "\n",
      "Epoch 98\n",
      "Batch:    88/88, Avg_loss: 1.490129, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.601761, metric: 0.858440\n",
      "\n",
      "Epoch 99\n",
      "Batch:    88/88, Avg_loss: 1.490224, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.607104, metric: 0.853365\n",
      "\n",
      "Epoch 100\n",
      "Batch:    88/88, Avg_loss: 1.490747, batch accuracy: 0.938775\n",
      "Validation:\n",
      "loss: 1.601881, metric: 0.859776\n",
      "\n",
      "Epoch 101\n",
      "Batch:    88/88, Avg_loss: 1.490626, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.600912, metric: 0.860310\n",
      "\n",
      "Epoch 102\n",
      "Batch:    88/88, Avg_loss: 1.490374, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.602193, metric: 0.859241\n",
      "\n",
      "Epoch 103\n",
      "Batch:    88/88, Avg_loss: 1.490282, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.599213, metric: 0.860310\n",
      "\n",
      "Epoch 104\n",
      "Batch:    88/88, Avg_loss: 1.490251, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.601713, metric: 0.858173\n",
      "\n",
      "Epoch 105\n",
      "Batch:    88/88, Avg_loss: 1.488792, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.601313, metric: 0.859241\n",
      "\n",
      "Epoch 106\n",
      "Batch:    88/88, Avg_loss: 1.488975, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.599116, metric: 0.860310\n",
      "\n",
      "Epoch 107\n",
      "Batch:    88/88, Avg_loss: 1.488855, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.601382, metric: 0.859241\n",
      "\n",
      "Epoch 108\n",
      "Batch:    88/88, Avg_loss: 1.489117, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.601143, metric: 0.861111\n",
      "\n",
      "Epoch 109\n",
      "Batch:    88/88, Avg_loss: 1.488584, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.597956, metric: 0.863515\n",
      "\n",
      "Epoch 110\n",
      "Batch:    88/88, Avg_loss: 1.487877, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.602641, metric: 0.858974\n",
      "\n",
      "Epoch 111\n",
      "Batch:    88/88, Avg_loss: 1.488972, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.594797, metric: 0.865919\n",
      "\n",
      "Epoch 112\n",
      "Batch:    88/88, Avg_loss: 1.487545, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.598362, metric: 0.864316\n",
      "\n",
      "Epoch 113\n",
      "Batch:    88/88, Avg_loss: 1.487571, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.598209, metric: 0.861645\n",
      "\n",
      "Epoch 114\n",
      "Batch:    88/88, Avg_loss: 1.487889, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.598417, metric: 0.860844\n",
      "\n",
      "Epoch 115\n",
      "Batch:    88/88, Avg_loss: 1.487669, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.597185, metric: 0.863782\n",
      "\n",
      "Epoch 116\n",
      "Batch:    88/88, Avg_loss: 1.487782, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.596057, metric: 0.864583\n",
      "\n",
      "Epoch 117\n",
      "Batch:    88/88, Avg_loss: 1.487529, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.596402, metric: 0.863782\n",
      "\n",
      "Epoch 118\n",
      "Batch:    88/88, Avg_loss: 1.488592, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.599468, metric: 0.861912\n",
      "\n",
      "Epoch 119\n",
      "Batch:    88/88, Avg_loss: 1.488667, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.596789, metric: 0.863515\n",
      "\n",
      "Epoch 120\n",
      "Batch:    88/88, Avg_loss: 1.488338, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.598565, metric: 0.861645\n",
      "\n",
      "Epoch 121\n",
      "Batch:    88/88, Avg_loss: 1.488657, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.601282, metric: 0.860043\n",
      "\n",
      "Epoch 122\n",
      "Batch:    88/88, Avg_loss: 1.488954, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.601575, metric: 0.858173\n",
      "\n",
      "Epoch 123\n",
      "Batch:    88/88, Avg_loss: 1.490217, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.598486, metric: 0.861111\n",
      "\n",
      "Epoch 124\n",
      "Batch:    88/88, Avg_loss: 1.490141, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.597381, metric: 0.864316\n",
      "\n",
      "Epoch 125\n",
      "Batch:    88/88, Avg_loss: 1.490095, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.597246, metric: 0.865652\n",
      "\n",
      "Epoch 126\n",
      "Batch:    88/88, Avg_loss: 1.488470, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.599419, metric: 0.861378\n",
      "\n",
      "Epoch 127\n",
      "Batch:    88/88, Avg_loss: 1.488700, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.595279, metric: 0.868056\n",
      "\n",
      "Epoch 128\n",
      "Batch:    88/88, Avg_loss: 1.487751, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.598729, metric: 0.862981\n",
      "\n",
      "Epoch 129\n",
      "Batch:    88/88, Avg_loss: 1.487426, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.597384, metric: 0.865385\n",
      "\n",
      "Epoch 130\n",
      "Batch:    88/88, Avg_loss: 1.487473, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.597300, metric: 0.864583\n",
      "\n",
      "Epoch 131\n",
      "Batch:    88/88, Avg_loss: 1.487529, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.600670, metric: 0.861912\n",
      "\n",
      "Epoch 132\n",
      "Batch:    88/88, Avg_loss: 1.487719, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.596957, metric: 0.864316\n",
      "\n",
      "Epoch 133\n",
      "Batch:    88/88, Avg_loss: 1.487781, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.600226, metric: 0.863782\n",
      "\n",
      "Epoch 134\n",
      "Batch:    88/88, Avg_loss: 1.487523, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.598249, metric: 0.864850\n",
      "\n",
      "Epoch 135\n",
      "Batch:    88/88, Avg_loss: 1.487896, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.599682, metric: 0.861111\n",
      "\n",
      "Epoch 136\n",
      "Batch:    88/88, Avg_loss: 1.488300, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.599981, metric: 0.862714\n",
      "\n",
      "Epoch 137\n",
      "Batch:    88/88, Avg_loss: 1.487603, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.598244, metric: 0.862447\n",
      "\n",
      "Epoch 138\n",
      "Batch:    88/88, Avg_loss: 1.487730, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.601421, metric: 0.861111\n",
      "\n",
      "Epoch 139\n",
      "Batch:    88/88, Avg_loss: 1.487952, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.601122, metric: 0.860043\n",
      "\n",
      "Epoch 140\n",
      "Batch:    88/88, Avg_loss: 1.487422, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.602144, metric: 0.860310\n",
      "\n",
      "Epoch 141\n",
      "Batch:    88/88, Avg_loss: 1.487713, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.599791, metric: 0.861111\n",
      "\n",
      "Epoch 142\n",
      "Batch:    88/88, Avg_loss: 1.487740, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.602892, metric: 0.857906\n",
      "\n",
      "Epoch 143\n",
      "Batch:    88/88, Avg_loss: 1.488566, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.599780, metric: 0.862981\n",
      "\n",
      "Epoch 144\n",
      "Batch:    88/88, Avg_loss: 1.488308, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.601207, metric: 0.861111\n",
      "\n",
      "Epoch 145\n",
      "Batch:    88/88, Avg_loss: 1.487868, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.600242, metric: 0.860844\n",
      "\n",
      "Epoch 146\n",
      "Batch:    88/88, Avg_loss: 1.487990, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.601299, metric: 0.859241\n",
      "\n",
      "Epoch 147\n",
      "Batch:    88/88, Avg_loss: 1.487821, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.605280, metric: 0.857105\n",
      "\n",
      "Epoch 148\n",
      "Batch:    88/88, Avg_loss: 1.488240, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.605873, metric: 0.854701\n",
      "\n",
      "Epoch 149\n",
      "Batch:    88/88, Avg_loss: 1.487298, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.604533, metric: 0.858173\n",
      "\n",
      "Epoch 150\n",
      "Batch:    88/88, Avg_loss: 1.488274, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.604209, metric: 0.858974\n",
      "\n",
      "Epoch 151\n",
      "Batch:    88/88, Avg_loss: 1.486844, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.601995, metric: 0.860844\n",
      "\n",
      "Epoch 152\n",
      "Batch:    88/88, Avg_loss: 1.486451, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.601204, metric: 0.860310\n",
      "\n",
      "Epoch 153\n",
      "Batch:    88/88, Avg_loss: 1.487551, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.599558, metric: 0.862447\n",
      "\n",
      "Epoch 154\n",
      "Batch:    88/88, Avg_loss: 1.487617, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.597562, metric: 0.862447\n",
      "\n",
      "Epoch 155\n",
      "Batch:    88/88, Avg_loss: 1.487252, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.597936, metric: 0.864049\n",
      "\n",
      "Epoch 156\n",
      "Batch:    88/88, Avg_loss: 1.486635, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.597605, metric: 0.864049\n",
      "\n",
      "Epoch 157\n",
      "Batch:    88/88, Avg_loss: 1.486618, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.600899, metric: 0.859241\n",
      "\n",
      "Epoch 158\n",
      "Batch:    88/88, Avg_loss: 1.486436, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.602068, metric: 0.859509\n",
      "\n",
      "Epoch 159\n",
      "Batch:    88/88, Avg_loss: 1.487337, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.600195, metric: 0.860844\n",
      "\n",
      "Epoch 160\n",
      "Batch:    88/88, Avg_loss: 1.487053, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.600867, metric: 0.863515\n",
      "\n",
      "Epoch 161\n",
      "Batch:    88/88, Avg_loss: 1.487381, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.599789, metric: 0.862981\n",
      "\n",
      "Epoch 162\n",
      "Batch:    88/88, Avg_loss: 1.487234, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.598619, metric: 0.863248\n",
      "\n",
      "Epoch 163\n",
      "Batch:    88/88, Avg_loss: 1.486953, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.597213, metric: 0.863248\n",
      "\n",
      "Epoch 164\n",
      "Batch:    88/88, Avg_loss: 1.486806, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.600403, metric: 0.860844\n",
      "\n",
      "Epoch 165\n",
      "Batch:    88/88, Avg_loss: 1.487280, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.600698, metric: 0.861111\n",
      "\n",
      "Epoch 166\n",
      "Batch:    88/88, Avg_loss: 1.487876, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.599217, metric: 0.863782\n",
      "\n",
      "Epoch 167\n",
      "Batch:    88/88, Avg_loss: 1.487552, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.602898, metric: 0.859509\n",
      "\n",
      "Epoch 168\n",
      "Batch:    88/88, Avg_loss: 1.487405, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.601774, metric: 0.860844\n",
      "\n",
      "Epoch 169\n",
      "Batch:    88/88, Avg_loss: 1.486431, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.600505, metric: 0.860577\n",
      "\n",
      "Epoch 170\n",
      "Batch:    88/88, Avg_loss: 1.486742, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.596722, metric: 0.863782\n",
      "\n",
      "Epoch 171\n",
      "Batch:    88/88, Avg_loss: 1.486961, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.598453, metric: 0.863248\n",
      "\n",
      "Epoch 172\n",
      "Batch:    88/88, Avg_loss: 1.487316, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.601255, metric: 0.861111\n",
      "\n",
      "Epoch 173\n",
      "Batch:    88/88, Avg_loss: 1.486810, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.598360, metric: 0.862714\n",
      "\n",
      "Epoch 174\n",
      "Batch:    88/88, Avg_loss: 1.486970, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.601076, metric: 0.860043\n",
      "\n",
      "Epoch 175\n",
      "Batch:    88/88, Avg_loss: 1.486692, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.600067, metric: 0.861645\n",
      "\n",
      "Epoch 176\n",
      "Batch:    88/88, Avg_loss: 1.486029, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.598595, metric: 0.863248\n",
      "\n",
      "Epoch 177\n",
      "Batch:    88/88, Avg_loss: 1.486247, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.597114, metric: 0.864049\n",
      "\n",
      "Epoch 178\n",
      "Batch:    88/88, Avg_loss: 1.486282, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.597307, metric: 0.863782\n",
      "\n",
      "Epoch 179\n",
      "Batch:    88/88, Avg_loss: 1.486596, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.599550, metric: 0.861378\n",
      "\n",
      "Epoch 180\n",
      "Batch:    88/88, Avg_loss: 1.487865, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.601243, metric: 0.861645\n",
      "\n",
      "Epoch 181\n",
      "Batch:    88/88, Avg_loss: 1.487371, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.597374, metric: 0.865652\n",
      "\n",
      "Epoch 182\n",
      "Batch:    88/88, Avg_loss: 1.486970, batch accuracy: 1.000000\n",
      "Validation:\n",
      "loss: 1.598449, metric: 0.862981\n",
      "\n",
      "Epoch 183\n",
      "Batch:    88/88, Avg_loss: 1.486609, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.597958, metric: 0.862981\n",
      "\n",
      "Epoch 184\n",
      "Batch:    88/88, Avg_loss: 1.487033, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.598304, metric: 0.864316\n",
      "\n",
      "Epoch 185\n",
      "Batch:    88/88, Avg_loss: 1.487048, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.599441, metric: 0.862714\n",
      "\n",
      "Epoch 186\n",
      "Batch:    88/88, Avg_loss: 1.486798, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.600078, metric: 0.861645\n",
      "\n",
      "Epoch 187\n",
      "Batch:    88/88, Avg_loss: 1.486208, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.597385, metric: 0.863782\n",
      "\n",
      "Epoch 188\n",
      "Batch:    88/88, Avg_loss: 1.485976, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.595889, metric: 0.866720\n",
      "\n",
      "Epoch 189\n",
      "Batch:    88/88, Avg_loss: 1.486769, batch accuracy: 0.959184\n",
      "Validation:\n",
      "loss: 1.596875, metric: 0.863782\n",
      "\n",
      "Epoch 190\n",
      "Batch:    88/88, Avg_loss: 1.487161, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.596040, metric: 0.866453\n",
      "\n",
      "Epoch 191\n",
      "Batch:    88/88, Avg_loss: 1.486675, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.597685, metric: 0.863515\n",
      "\n",
      "Epoch 192\n",
      "Batch:    88/88, Avg_loss: 1.486777, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.596880, metric: 0.863782\n",
      "\n",
      "Epoch 193\n",
      "Batch:    88/88, Avg_loss: 1.486821, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.598580, metric: 0.862447\n",
      "\n",
      "Epoch 194\n",
      "Batch:    88/88, Avg_loss: 1.487439, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.598627, metric: 0.864049\n",
      "\n",
      "Epoch 195\n",
      "Batch:    88/88, Avg_loss: 1.486241, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.597262, metric: 0.864583\n",
      "\n",
      "Epoch 196\n",
      "Batch:    88/88, Avg_loss: 1.486518, batch accuracy: 0.979592\n",
      "Validation:\n",
      "loss: 1.599525, metric: 0.861912\n",
      "\n",
      "Epoch 197\n",
      "Batch:    88/88, Avg_loss: 1.487479, batch accuracy: 0.969388\n",
      "Validation:\n",
      "loss: 1.594470, metric: 0.865919\n",
      "\n",
      "Epoch 198\n",
      "Batch:    88/88, Avg_loss: 1.487255, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.595170, metric: 0.868056\n",
      "\n",
      "Epoch 199\n",
      "Batch:    88/88, Avg_loss: 1.486335, batch accuracy: 0.948980\n",
      "Validation:\n",
      "loss: 1.593957, metric: 0.867788\n",
      "\n",
      "Epoch 200\n",
      "Batch:    88/88, Avg_loss: 1.486202, batch accuracy: 0.989796\n",
      "Validation:\n",
      "loss: 1.595761, metric: 0.865385\n",
      "\n",
      "Testing:\n",
      "loss: 1.587034, metric: 0.874266\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model15 = Network_batchnorm([28*28, 1000, 500, 250, 100, 10]).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model15.parameters(), lr = 0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"\\nEpoch {t+1}\")\n",
    "    train(train_loader, model15, loss_fn, optimizer, accuracy)\n",
    "    print('Validation:')\n",
    "    test (valid_loader, model15, loss_fn, accuracy)\n",
    "print('\\nTesting:')\n",
    "test_accuracy = test (test_loader, model15, loss_fn, accuracy)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, the highest accuracy achieved was 0.874266. Despite experimenting with weight decay and training for up to 200 epochs, the test accuracy did not surpass 0.87. Therefore, model15 remains the best-performing model for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt",
   "language": "python",
   "name": "opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
